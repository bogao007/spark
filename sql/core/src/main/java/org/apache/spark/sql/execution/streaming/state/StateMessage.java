// Generated by the protocol buffer compiler.  DO NOT EDIT!
// source: org/apache/spark/sql/execution/streaming/StateMessage.proto

// Protobuf Java Version: 3.25.2
package org.apache.spark.sql.execution.streaming.state;

public final class StateMessage {
  private StateMessage() {}
  public static void registerAllExtensions(
      com.google.protobuf.ExtensionRegistryLite registry) {
  }

  public static void registerAllExtensions(
      com.google.protobuf.ExtensionRegistry registry) {
    registerAllExtensions(
        (com.google.protobuf.ExtensionRegistryLite) registry);
  }
  /**
   * Protobuf enum {@code org.apache.spark.sql.execution.streaming.state.HandleState}
   */
  public enum HandleState
      implements com.google.protobuf.ProtocolMessageEnum {
    /**
     * <code>CREATED = 0;</code>
     */
    CREATED(0),
    /**
     * <code>INITIALIZED = 1;</code>
     */
    INITIALIZED(1),
    /**
     * <code>DATA_PROCESSED = 2;</code>
     */
    DATA_PROCESSED(2),
    /**
     * <code>CLOSED = 3;</code>
     */
    CLOSED(3),
    UNRECOGNIZED(-1),
    ;

    /**
     * <code>CREATED = 0;</code>
     */
    public static final int CREATED_VALUE = 0;
    /**
     * <code>INITIALIZED = 1;</code>
     */
    public static final int INITIALIZED_VALUE = 1;
    /**
     * <code>DATA_PROCESSED = 2;</code>
     */
    public static final int DATA_PROCESSED_VALUE = 2;
    /**
     * <code>CLOSED = 3;</code>
     */
    public static final int CLOSED_VALUE = 3;


    public final int getNumber() {
      if (this == UNRECOGNIZED) {
        throw new java.lang.IllegalArgumentException(
            "Can't get the number of an unknown enum value.");
      }
      return value;
    }

    /**
     * @param value The numeric wire value of the corresponding enum entry.
     * @return The enum associated with the given numeric wire value.
     * @deprecated Use {@link #forNumber(int)} instead.
     */
    @java.lang.Deprecated
    public static HandleState valueOf(int value) {
      return forNumber(value);
    }

    /**
     * @param value The numeric wire value of the corresponding enum entry.
     * @return The enum associated with the given numeric wire value.
     */
    public static HandleState forNumber(int value) {
      switch (value) {
        case 0: return CREATED;
        case 1: return INITIALIZED;
        case 2: return DATA_PROCESSED;
        case 3: return CLOSED;
        default: return null;
      }
    }

    public static com.google.protobuf.Internal.EnumLiteMap<HandleState>
        internalGetValueMap() {
      return internalValueMap;
    }
    private static final com.google.protobuf.Internal.EnumLiteMap<
        HandleState> internalValueMap =
          new com.google.protobuf.Internal.EnumLiteMap<HandleState>() {
            public HandleState findValueByNumber(int number) {
              return HandleState.forNumber(number);
            }
          };

    public final com.google.protobuf.Descriptors.EnumValueDescriptor
        getValueDescriptor() {
      if (this == UNRECOGNIZED) {
        throw new java.lang.IllegalStateException(
            "Can't get the descriptor of an unrecognized enum value.");
      }
      return getDescriptor().getValues().get(ordinal());
    }
    public final com.google.protobuf.Descriptors.EnumDescriptor
        getDescriptorForType() {
      return getDescriptor();
    }
    public static final com.google.protobuf.Descriptors.EnumDescriptor
        getDescriptor() {
      return org.apache.spark.sql.execution.streaming.state.StateMessage.getDescriptor().getEnumTypes().get(0);
    }

    private static final HandleState[] VALUES = values();

    public static HandleState valueOf(
        com.google.protobuf.Descriptors.EnumValueDescriptor desc) {
      if (desc.getType() != getDescriptor()) {
        throw new java.lang.IllegalArgumentException(
          "EnumValueDescriptor is not for this type.");
      }
      if (desc.getIndex() == -1) {
        return UNRECOGNIZED;
      }
      return VALUES[desc.getIndex()];
    }

    private final int value;

    private HandleState(int value) {
      this.value = value;
    }

    // @@protoc_insertion_point(enum_scope:org.apache.spark.sql.execution.streaming.state.HandleState)
  }

  public interface StateRequestOrBuilder extends
      // @@protoc_insertion_point(interface_extends:org.apache.spark.sql.execution.streaming.state.StateRequest)
      com.google.protobuf.MessageOrBuilder {

    /**
     * <code>int32 version = 1;</code>
     * @return The version.
     */
    int getVersion();

    /**
     * <code>.org.apache.spark.sql.execution.streaming.state.StatefulProcessorHandleCall statefulProcessorHandleCall = 2;</code>
     * @return Whether the statefulProcessorHandleCall field is set.
     */
    boolean hasStatefulProcessorHandleCall();
    /**
     * <code>.org.apache.spark.sql.execution.streaming.state.StatefulProcessorHandleCall statefulProcessorHandleCall = 2;</code>
     * @return The statefulProcessorHandleCall.
     */
    org.apache.spark.sql.execution.streaming.state.StateMessage.StatefulProcessorHandleCall getStatefulProcessorHandleCall();
    /**
     * <code>.org.apache.spark.sql.execution.streaming.state.StatefulProcessorHandleCall statefulProcessorHandleCall = 2;</code>
     */
    org.apache.spark.sql.execution.streaming.state.StateMessage.StatefulProcessorHandleCallOrBuilder getStatefulProcessorHandleCallOrBuilder();

    /**
     * <code>.org.apache.spark.sql.execution.streaming.state.ValueStateCall valueStateCall = 3;</code>
     * @return Whether the valueStateCall field is set.
     */
    boolean hasValueStateCall();
    /**
     * <code>.org.apache.spark.sql.execution.streaming.state.ValueStateCall valueStateCall = 3;</code>
     * @return The valueStateCall.
     */
    org.apache.spark.sql.execution.streaming.state.StateMessage.ValueStateCall getValueStateCall();
    /**
     * <code>.org.apache.spark.sql.execution.streaming.state.ValueStateCall valueStateCall = 3;</code>
     */
    org.apache.spark.sql.execution.streaming.state.StateMessage.ValueStateCallOrBuilder getValueStateCallOrBuilder();

    org.apache.spark.sql.execution.streaming.state.StateMessage.StateRequest.MethodCase getMethodCase();
  }
  /**
   * Protobuf type {@code org.apache.spark.sql.execution.streaming.state.StateRequest}
   */
  public static final class StateRequest extends
      com.google.protobuf.GeneratedMessageV3 implements
      // @@protoc_insertion_point(message_implements:org.apache.spark.sql.execution.streaming.state.StateRequest)
      StateRequestOrBuilder {
  private static final long serialVersionUID = 0L;
    // Use StateRequest.newBuilder() to construct.
    private StateRequest(com.google.protobuf.GeneratedMessageV3.Builder<?> builder) {
      super(builder);
    }
    private StateRequest() {
    }

    @java.lang.Override
    @SuppressWarnings({"unused"})
    protected java.lang.Object newInstance(
        UnusedPrivateParameter unused) {
      return new StateRequest();
    }

    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.spark.sql.execution.streaming.state.StateMessage.internal_static_org_apache_spark_sql_execution_streaming_state_StateRequest_descriptor;
    }

    @java.lang.Override
    protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.spark.sql.execution.streaming.state.StateMessage.internal_static_org_apache_spark_sql_execution_streaming_state_StateRequest_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.spark.sql.execution.streaming.state.StateMessage.StateRequest.class, org.apache.spark.sql.execution.streaming.state.StateMessage.StateRequest.Builder.class);
    }

    private int methodCase_ = 0;
    @SuppressWarnings("serial")
    private java.lang.Object method_;
    public enum MethodCase
        implements com.google.protobuf.Internal.EnumLite,
            com.google.protobuf.AbstractMessage.InternalOneOfEnum {
      STATEFULPROCESSORHANDLECALL(2),
      VALUESTATECALL(3),
      METHOD_NOT_SET(0);
      private final int value;
      private MethodCase(int value) {
        this.value = value;
      }
      /**
       * @param value The number of the enum to look for.
       * @return The enum associated with the given number.
       * @deprecated Use {@link #forNumber(int)} instead.
       */
      @java.lang.Deprecated
      public static MethodCase valueOf(int value) {
        return forNumber(value);
      }

      public static MethodCase forNumber(int value) {
        switch (value) {
          case 2: return STATEFULPROCESSORHANDLECALL;
          case 3: return VALUESTATECALL;
          case 0: return METHOD_NOT_SET;
          default: return null;
        }
      }
      public int getNumber() {
        return this.value;
      }
    };

    public MethodCase
    getMethodCase() {
      return MethodCase.forNumber(
          methodCase_);
    }

    public static final int VERSION_FIELD_NUMBER = 1;
    private int version_ = 0;
    /**
     * <code>int32 version = 1;</code>
     * @return The version.
     */
    @java.lang.Override
    public int getVersion() {
      return version_;
    }

    public static final int STATEFULPROCESSORHANDLECALL_FIELD_NUMBER = 2;
    /**
     * <code>.org.apache.spark.sql.execution.streaming.state.StatefulProcessorHandleCall statefulProcessorHandleCall = 2;</code>
     * @return Whether the statefulProcessorHandleCall field is set.
     */
    @java.lang.Override
    public boolean hasStatefulProcessorHandleCall() {
      return methodCase_ == 2;
    }
    /**
     * <code>.org.apache.spark.sql.execution.streaming.state.StatefulProcessorHandleCall statefulProcessorHandleCall = 2;</code>
     * @return The statefulProcessorHandleCall.
     */
    @java.lang.Override
    public org.apache.spark.sql.execution.streaming.state.StateMessage.StatefulProcessorHandleCall getStatefulProcessorHandleCall() {
      if (methodCase_ == 2) {
         return (org.apache.spark.sql.execution.streaming.state.StateMessage.StatefulProcessorHandleCall) method_;
      }
      return org.apache.spark.sql.execution.streaming.state.StateMessage.StatefulProcessorHandleCall.getDefaultInstance();
    }
    /**
     * <code>.org.apache.spark.sql.execution.streaming.state.StatefulProcessorHandleCall statefulProcessorHandleCall = 2;</code>
     */
    @java.lang.Override
    public org.apache.spark.sql.execution.streaming.state.StateMessage.StatefulProcessorHandleCallOrBuilder getStatefulProcessorHandleCallOrBuilder() {
      if (methodCase_ == 2) {
         return (org.apache.spark.sql.execution.streaming.state.StateMessage.StatefulProcessorHandleCall) method_;
      }
      return org.apache.spark.sql.execution.streaming.state.StateMessage.StatefulProcessorHandleCall.getDefaultInstance();
    }

    public static final int VALUESTATECALL_FIELD_NUMBER = 3;
    /**
     * <code>.org.apache.spark.sql.execution.streaming.state.ValueStateCall valueStateCall = 3;</code>
     * @return Whether the valueStateCall field is set.
     */
    @java.lang.Override
    public boolean hasValueStateCall() {
      return methodCase_ == 3;
    }
    /**
     * <code>.org.apache.spark.sql.execution.streaming.state.ValueStateCall valueStateCall = 3;</code>
     * @return The valueStateCall.
     */
    @java.lang.Override
    public org.apache.spark.sql.execution.streaming.state.StateMessage.ValueStateCall getValueStateCall() {
      if (methodCase_ == 3) {
         return (org.apache.spark.sql.execution.streaming.state.StateMessage.ValueStateCall) method_;
      }
      return org.apache.spark.sql.execution.streaming.state.StateMessage.ValueStateCall.getDefaultInstance();
    }
    /**
     * <code>.org.apache.spark.sql.execution.streaming.state.ValueStateCall valueStateCall = 3;</code>
     */
    @java.lang.Override
    public org.apache.spark.sql.execution.streaming.state.StateMessage.ValueStateCallOrBuilder getValueStateCallOrBuilder() {
      if (methodCase_ == 3) {
         return (org.apache.spark.sql.execution.streaming.state.StateMessage.ValueStateCall) method_;
      }
      return org.apache.spark.sql.execution.streaming.state.StateMessage.ValueStateCall.getDefaultInstance();
    }

    private byte memoizedIsInitialized = -1;
    @java.lang.Override
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized == 1) return true;
      if (isInitialized == 0) return false;

      memoizedIsInitialized = 1;
      return true;
    }

    @java.lang.Override
    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      if (version_ != 0) {
        output.writeInt32(1, version_);
      }
      if (methodCase_ == 2) {
        output.writeMessage(2, (org.apache.spark.sql.execution.streaming.state.StateMessage.StatefulProcessorHandleCall) method_);
      }
      if (methodCase_ == 3) {
        output.writeMessage(3, (org.apache.spark.sql.execution.streaming.state.StateMessage.ValueStateCall) method_);
      }
      getUnknownFields().writeTo(output);
    }

    @java.lang.Override
    public int getSerializedSize() {
      int size = memoizedSize;
      if (size != -1) return size;

      size = 0;
      if (version_ != 0) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt32Size(1, version_);
      }
      if (methodCase_ == 2) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(2, (org.apache.spark.sql.execution.streaming.state.StateMessage.StatefulProcessorHandleCall) method_);
      }
      if (methodCase_ == 3) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(3, (org.apache.spark.sql.execution.streaming.state.StateMessage.ValueStateCall) method_);
      }
      size += getUnknownFields().getSerializedSize();
      memoizedSize = size;
      return size;
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.spark.sql.execution.streaming.state.StateMessage.StateRequest)) {
        return super.equals(obj);
      }
      org.apache.spark.sql.execution.streaming.state.StateMessage.StateRequest other = (org.apache.spark.sql.execution.streaming.state.StateMessage.StateRequest) obj;

      if (getVersion()
          != other.getVersion()) return false;
      if (!getMethodCase().equals(other.getMethodCase())) return false;
      switch (methodCase_) {
        case 2:
          if (!getStatefulProcessorHandleCall()
              .equals(other.getStatefulProcessorHandleCall())) return false;
          break;
        case 3:
          if (!getValueStateCall()
              .equals(other.getValueStateCall())) return false;
          break;
        case 0:
        default:
      }
      if (!getUnknownFields().equals(other.getUnknownFields())) return false;
      return true;
    }

    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptor().hashCode();
      hash = (37 * hash) + VERSION_FIELD_NUMBER;
      hash = (53 * hash) + getVersion();
      switch (methodCase_) {
        case 2:
          hash = (37 * hash) + STATEFULPROCESSORHANDLECALL_FIELD_NUMBER;
          hash = (53 * hash) + getStatefulProcessorHandleCall().hashCode();
          break;
        case 3:
          hash = (37 * hash) + VALUESTATECALL_FIELD_NUMBER;
          hash = (53 * hash) + getValueStateCall().hashCode();
          break;
        case 0:
        default:
      }
      hash = (29 * hash) + getUnknownFields().hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.spark.sql.execution.streaming.state.StateMessage.StateRequest parseFrom(
        java.nio.ByteBuffer data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.spark.sql.execution.streaming.state.StateMessage.StateRequest parseFrom(
        java.nio.ByteBuffer data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.spark.sql.execution.streaming.state.StateMessage.StateRequest parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.spark.sql.execution.streaming.state.StateMessage.StateRequest parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.spark.sql.execution.streaming.state.StateMessage.StateRequest parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.spark.sql.execution.streaming.state.StateMessage.StateRequest parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.spark.sql.execution.streaming.state.StateMessage.StateRequest parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.spark.sql.execution.streaming.state.StateMessage.StateRequest parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }

    public static org.apache.spark.sql.execution.streaming.state.StateMessage.StateRequest parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input);
    }

    public static org.apache.spark.sql.execution.streaming.state.StateMessage.StateRequest parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.spark.sql.execution.streaming.state.StateMessage.StateRequest parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.spark.sql.execution.streaming.state.StateMessage.StateRequest parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }

    @java.lang.Override
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder() {
      return DEFAULT_INSTANCE.toBuilder();
    }
    public static Builder newBuilder(org.apache.spark.sql.execution.streaming.state.StateMessage.StateRequest prototype) {
      return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
    }
    @java.lang.Override
    public Builder toBuilder() {
      return this == DEFAULT_INSTANCE
          ? new Builder() : new Builder().mergeFrom(this);
    }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code org.apache.spark.sql.execution.streaming.state.StateRequest}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessageV3.Builder<Builder> implements
        // @@protoc_insertion_point(builder_implements:org.apache.spark.sql.execution.streaming.state.StateRequest)
        org.apache.spark.sql.execution.streaming.state.StateMessage.StateRequestOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.spark.sql.execution.streaming.state.StateMessage.internal_static_org_apache_spark_sql_execution_streaming_state_StateRequest_descriptor;
      }

      @java.lang.Override
      protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.spark.sql.execution.streaming.state.StateMessage.internal_static_org_apache_spark_sql_execution_streaming_state_StateRequest_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.spark.sql.execution.streaming.state.StateMessage.StateRequest.class, org.apache.spark.sql.execution.streaming.state.StateMessage.StateRequest.Builder.class);
      }

      // Construct using org.apache.spark.sql.execution.streaming.state.StateMessage.StateRequest.newBuilder()
      private Builder() {

      }

      private Builder(
          com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
        super(parent);

      }
      @java.lang.Override
      public Builder clear() {
        super.clear();
        bitField0_ = 0;
        version_ = 0;
        if (statefulProcessorHandleCallBuilder_ != null) {
          statefulProcessorHandleCallBuilder_.clear();
        }
        if (valueStateCallBuilder_ != null) {
          valueStateCallBuilder_.clear();
        }
        methodCase_ = 0;
        method_ = null;
        return this;
      }

      @java.lang.Override
      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.spark.sql.execution.streaming.state.StateMessage.internal_static_org_apache_spark_sql_execution_streaming_state_StateRequest_descriptor;
      }

      @java.lang.Override
      public org.apache.spark.sql.execution.streaming.state.StateMessage.StateRequest getDefaultInstanceForType() {
        return org.apache.spark.sql.execution.streaming.state.StateMessage.StateRequest.getDefaultInstance();
      }

      @java.lang.Override
      public org.apache.spark.sql.execution.streaming.state.StateMessage.StateRequest build() {
        org.apache.spark.sql.execution.streaming.state.StateMessage.StateRequest result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      @java.lang.Override
      public org.apache.spark.sql.execution.streaming.state.StateMessage.StateRequest buildPartial() {
        org.apache.spark.sql.execution.streaming.state.StateMessage.StateRequest result = new org.apache.spark.sql.execution.streaming.state.StateMessage.StateRequest(this);
        if (bitField0_ != 0) { buildPartial0(result); }
        buildPartialOneofs(result);
        onBuilt();
        return result;
      }

      private void buildPartial0(org.apache.spark.sql.execution.streaming.state.StateMessage.StateRequest result) {
        int from_bitField0_ = bitField0_;
        if (((from_bitField0_ & 0x00000001) != 0)) {
          result.version_ = version_;
        }
      }

      private void buildPartialOneofs(org.apache.spark.sql.execution.streaming.state.StateMessage.StateRequest result) {
        result.methodCase_ = methodCase_;
        result.method_ = this.method_;
        if (methodCase_ == 2 &&
            statefulProcessorHandleCallBuilder_ != null) {
          result.method_ = statefulProcessorHandleCallBuilder_.build();
        }
        if (methodCase_ == 3 &&
            valueStateCallBuilder_ != null) {
          result.method_ = valueStateCallBuilder_.build();
        }
      }

      @java.lang.Override
      public Builder clone() {
        return super.clone();
      }
      @java.lang.Override
      public Builder setField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.setField(field, value);
      }
      @java.lang.Override
      public Builder clearField(
          com.google.protobuf.Descriptors.FieldDescriptor field) {
        return super.clearField(field);
      }
      @java.lang.Override
      public Builder clearOneof(
          com.google.protobuf.Descriptors.OneofDescriptor oneof) {
        return super.clearOneof(oneof);
      }
      @java.lang.Override
      public Builder setRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          int index, java.lang.Object value) {
        return super.setRepeatedField(field, index, value);
      }
      @java.lang.Override
      public Builder addRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.addRepeatedField(field, value);
      }
      @java.lang.Override
      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.spark.sql.execution.streaming.state.StateMessage.StateRequest) {
          return mergeFrom((org.apache.spark.sql.execution.streaming.state.StateMessage.StateRequest)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.spark.sql.execution.streaming.state.StateMessage.StateRequest other) {
        if (other == org.apache.spark.sql.execution.streaming.state.StateMessage.StateRequest.getDefaultInstance()) return this;
        if (other.getVersion() != 0) {
          setVersion(other.getVersion());
        }
        switch (other.getMethodCase()) {
          case STATEFULPROCESSORHANDLECALL: {
            mergeStatefulProcessorHandleCall(other.getStatefulProcessorHandleCall());
            break;
          }
          case VALUESTATECALL: {
            mergeValueStateCall(other.getValueStateCall());
            break;
          }
          case METHOD_NOT_SET: {
            break;
          }
        }
        this.mergeUnknownFields(other.getUnknownFields());
        onChanged();
        return this;
      }

      @java.lang.Override
      public final boolean isInitialized() {
        return true;
      }

      @java.lang.Override
      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        if (extensionRegistry == null) {
          throw new java.lang.NullPointerException();
        }
        try {
          boolean done = false;
          while (!done) {
            int tag = input.readTag();
            switch (tag) {
              case 0:
                done = true;
                break;
              case 8: {
                version_ = input.readInt32();
                bitField0_ |= 0x00000001;
                break;
              } // case 8
              case 18: {
                input.readMessage(
                    getStatefulProcessorHandleCallFieldBuilder().getBuilder(),
                    extensionRegistry);
                methodCase_ = 2;
                break;
              } // case 18
              case 26: {
                input.readMessage(
                    getValueStateCallFieldBuilder().getBuilder(),
                    extensionRegistry);
                methodCase_ = 3;
                break;
              } // case 26
              default: {
                if (!super.parseUnknownField(input, extensionRegistry, tag)) {
                  done = true; // was an endgroup tag
                }
                break;
              } // default:
            } // switch (tag)
          } // while (!done)
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          throw e.unwrapIOException();
        } finally {
          onChanged();
        } // finally
        return this;
      }
      private int methodCase_ = 0;
      private java.lang.Object method_;
      public MethodCase
          getMethodCase() {
        return MethodCase.forNumber(
            methodCase_);
      }

      public Builder clearMethod() {
        methodCase_ = 0;
        method_ = null;
        onChanged();
        return this;
      }

      private int bitField0_;

      private int version_ ;
      /**
       * <code>int32 version = 1;</code>
       * @return The version.
       */
      @java.lang.Override
      public int getVersion() {
        return version_;
      }
      /**
       * <code>int32 version = 1;</code>
       * @param value The version to set.
       * @return This builder for chaining.
       */
      public Builder setVersion(int value) {

        version_ = value;
        bitField0_ |= 0x00000001;
        onChanged();
        return this;
      }
      /**
       * <code>int32 version = 1;</code>
       * @return This builder for chaining.
       */
      public Builder clearVersion() {
        bitField0_ = (bitField0_ & ~0x00000001);
        version_ = 0;
        onChanged();
        return this;
      }

      private com.google.protobuf.SingleFieldBuilderV3<
          org.apache.spark.sql.execution.streaming.state.StateMessage.StatefulProcessorHandleCall, org.apache.spark.sql.execution.streaming.state.StateMessage.StatefulProcessorHandleCall.Builder, org.apache.spark.sql.execution.streaming.state.StateMessage.StatefulProcessorHandleCallOrBuilder> statefulProcessorHandleCallBuilder_;
      /**
       * <code>.org.apache.spark.sql.execution.streaming.state.StatefulProcessorHandleCall statefulProcessorHandleCall = 2;</code>
       * @return Whether the statefulProcessorHandleCall field is set.
       */
      @java.lang.Override
      public boolean hasStatefulProcessorHandleCall() {
        return methodCase_ == 2;
      }
      /**
       * <code>.org.apache.spark.sql.execution.streaming.state.StatefulProcessorHandleCall statefulProcessorHandleCall = 2;</code>
       * @return The statefulProcessorHandleCall.
       */
      @java.lang.Override
      public org.apache.spark.sql.execution.streaming.state.StateMessage.StatefulProcessorHandleCall getStatefulProcessorHandleCall() {
        if (statefulProcessorHandleCallBuilder_ == null) {
          if (methodCase_ == 2) {
            return (org.apache.spark.sql.execution.streaming.state.StateMessage.StatefulProcessorHandleCall) method_;
          }
          return org.apache.spark.sql.execution.streaming.state.StateMessage.StatefulProcessorHandleCall.getDefaultInstance();
        } else {
          if (methodCase_ == 2) {
            return statefulProcessorHandleCallBuilder_.getMessage();
          }
          return org.apache.spark.sql.execution.streaming.state.StateMessage.StatefulProcessorHandleCall.getDefaultInstance();
        }
      }
      /**
       * <code>.org.apache.spark.sql.execution.streaming.state.StatefulProcessorHandleCall statefulProcessorHandleCall = 2;</code>
       */
      public Builder setStatefulProcessorHandleCall(org.apache.spark.sql.execution.streaming.state.StateMessage.StatefulProcessorHandleCall value) {
        if (statefulProcessorHandleCallBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          method_ = value;
          onChanged();
        } else {
          statefulProcessorHandleCallBuilder_.setMessage(value);
        }
        methodCase_ = 2;
        return this;
      }
      /**
       * <code>.org.apache.spark.sql.execution.streaming.state.StatefulProcessorHandleCall statefulProcessorHandleCall = 2;</code>
       */
      public Builder setStatefulProcessorHandleCall(
          org.apache.spark.sql.execution.streaming.state.StateMessage.StatefulProcessorHandleCall.Builder builderForValue) {
        if (statefulProcessorHandleCallBuilder_ == null) {
          method_ = builderForValue.build();
          onChanged();
        } else {
          statefulProcessorHandleCallBuilder_.setMessage(builderForValue.build());
        }
        methodCase_ = 2;
        return this;
      }
      /**
       * <code>.org.apache.spark.sql.execution.streaming.state.StatefulProcessorHandleCall statefulProcessorHandleCall = 2;</code>
       */
      public Builder mergeStatefulProcessorHandleCall(org.apache.spark.sql.execution.streaming.state.StateMessage.StatefulProcessorHandleCall value) {
        if (statefulProcessorHandleCallBuilder_ == null) {
          if (methodCase_ == 2 &&
              method_ != org.apache.spark.sql.execution.streaming.state.StateMessage.StatefulProcessorHandleCall.getDefaultInstance()) {
            method_ = org.apache.spark.sql.execution.streaming.state.StateMessage.StatefulProcessorHandleCall.newBuilder((org.apache.spark.sql.execution.streaming.state.StateMessage.StatefulProcessorHandleCall) method_)
                .mergeFrom(value).buildPartial();
          } else {
            method_ = value;
          }
          onChanged();
        } else {
          if (methodCase_ == 2) {
            statefulProcessorHandleCallBuilder_.mergeFrom(value);
          } else {
            statefulProcessorHandleCallBuilder_.setMessage(value);
          }
        }
        methodCase_ = 2;
        return this;
      }
      /**
       * <code>.org.apache.spark.sql.execution.streaming.state.StatefulProcessorHandleCall statefulProcessorHandleCall = 2;</code>
       */
      public Builder clearStatefulProcessorHandleCall() {
        if (statefulProcessorHandleCallBuilder_ == null) {
          if (methodCase_ == 2) {
            methodCase_ = 0;
            method_ = null;
            onChanged();
          }
        } else {
          if (methodCase_ == 2) {
            methodCase_ = 0;
            method_ = null;
          }
          statefulProcessorHandleCallBuilder_.clear();
        }
        return this;
      }
      /**
       * <code>.org.apache.spark.sql.execution.streaming.state.StatefulProcessorHandleCall statefulProcessorHandleCall = 2;</code>
       */
      public org.apache.spark.sql.execution.streaming.state.StateMessage.StatefulProcessorHandleCall.Builder getStatefulProcessorHandleCallBuilder() {
        return getStatefulProcessorHandleCallFieldBuilder().getBuilder();
      }
      /**
       * <code>.org.apache.spark.sql.execution.streaming.state.StatefulProcessorHandleCall statefulProcessorHandleCall = 2;</code>
       */
      @java.lang.Override
      public org.apache.spark.sql.execution.streaming.state.StateMessage.StatefulProcessorHandleCallOrBuilder getStatefulProcessorHandleCallOrBuilder() {
        if ((methodCase_ == 2) && (statefulProcessorHandleCallBuilder_ != null)) {
          return statefulProcessorHandleCallBuilder_.getMessageOrBuilder();
        } else {
          if (methodCase_ == 2) {
            return (org.apache.spark.sql.execution.streaming.state.StateMessage.StatefulProcessorHandleCall) method_;
          }
          return org.apache.spark.sql.execution.streaming.state.StateMessage.StatefulProcessorHandleCall.getDefaultInstance();
        }
      }
      /**
       * <code>.org.apache.spark.sql.execution.streaming.state.StatefulProcessorHandleCall statefulProcessorHandleCall = 2;</code>
       */
      private com.google.protobuf.SingleFieldBuilderV3<
          org.apache.spark.sql.execution.streaming.state.StateMessage.StatefulProcessorHandleCall, org.apache.spark.sql.execution.streaming.state.StateMessage.StatefulProcessorHandleCall.Builder, org.apache.spark.sql.execution.streaming.state.StateMessage.StatefulProcessorHandleCallOrBuilder> 
          getStatefulProcessorHandleCallFieldBuilder() {
        if (statefulProcessorHandleCallBuilder_ == null) {
          if (!(methodCase_ == 2)) {
            method_ = org.apache.spark.sql.execution.streaming.state.StateMessage.StatefulProcessorHandleCall.getDefaultInstance();
          }
          statefulProcessorHandleCallBuilder_ = new com.google.protobuf.SingleFieldBuilderV3<
              org.apache.spark.sql.execution.streaming.state.StateMessage.StatefulProcessorHandleCall, org.apache.spark.sql.execution.streaming.state.StateMessage.StatefulProcessorHandleCall.Builder, org.apache.spark.sql.execution.streaming.state.StateMessage.StatefulProcessorHandleCallOrBuilder>(
                  (org.apache.spark.sql.execution.streaming.state.StateMessage.StatefulProcessorHandleCall) method_,
                  getParentForChildren(),
                  isClean());
          method_ = null;
        }
        methodCase_ = 2;
        onChanged();
        return statefulProcessorHandleCallBuilder_;
      }

      private com.google.protobuf.SingleFieldBuilderV3<
          org.apache.spark.sql.execution.streaming.state.StateMessage.ValueStateCall, org.apache.spark.sql.execution.streaming.state.StateMessage.ValueStateCall.Builder, org.apache.spark.sql.execution.streaming.state.StateMessage.ValueStateCallOrBuilder> valueStateCallBuilder_;
      /**
       * <code>.org.apache.spark.sql.execution.streaming.state.ValueStateCall valueStateCall = 3;</code>
       * @return Whether the valueStateCall field is set.
       */
      @java.lang.Override
      public boolean hasValueStateCall() {
        return methodCase_ == 3;
      }
      /**
       * <code>.org.apache.spark.sql.execution.streaming.state.ValueStateCall valueStateCall = 3;</code>
       * @return The valueStateCall.
       */
      @java.lang.Override
      public org.apache.spark.sql.execution.streaming.state.StateMessage.ValueStateCall getValueStateCall() {
        if (valueStateCallBuilder_ == null) {
          if (methodCase_ == 3) {
            return (org.apache.spark.sql.execution.streaming.state.StateMessage.ValueStateCall) method_;
          }
          return org.apache.spark.sql.execution.streaming.state.StateMessage.ValueStateCall.getDefaultInstance();
        } else {
          if (methodCase_ == 3) {
            return valueStateCallBuilder_.getMessage();
          }
          return org.apache.spark.sql.execution.streaming.state.StateMessage.ValueStateCall.getDefaultInstance();
        }
      }
      /**
       * <code>.org.apache.spark.sql.execution.streaming.state.ValueStateCall valueStateCall = 3;</code>
       */
      public Builder setValueStateCall(org.apache.spark.sql.execution.streaming.state.StateMessage.ValueStateCall value) {
        if (valueStateCallBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          method_ = value;
          onChanged();
        } else {
          valueStateCallBuilder_.setMessage(value);
        }
        methodCase_ = 3;
        return this;
      }
      /**
       * <code>.org.apache.spark.sql.execution.streaming.state.ValueStateCall valueStateCall = 3;</code>
       */
      public Builder setValueStateCall(
          org.apache.spark.sql.execution.streaming.state.StateMessage.ValueStateCall.Builder builderForValue) {
        if (valueStateCallBuilder_ == null) {
          method_ = builderForValue.build();
          onChanged();
        } else {
          valueStateCallBuilder_.setMessage(builderForValue.build());
        }
        methodCase_ = 3;
        return this;
      }
      /**
       * <code>.org.apache.spark.sql.execution.streaming.state.ValueStateCall valueStateCall = 3;</code>
       */
      public Builder mergeValueStateCall(org.apache.spark.sql.execution.streaming.state.StateMessage.ValueStateCall value) {
        if (valueStateCallBuilder_ == null) {
          if (methodCase_ == 3 &&
              method_ != org.apache.spark.sql.execution.streaming.state.StateMessage.ValueStateCall.getDefaultInstance()) {
            method_ = org.apache.spark.sql.execution.streaming.state.StateMessage.ValueStateCall.newBuilder((org.apache.spark.sql.execution.streaming.state.StateMessage.ValueStateCall) method_)
                .mergeFrom(value).buildPartial();
          } else {
            method_ = value;
          }
          onChanged();
        } else {
          if (methodCase_ == 3) {
            valueStateCallBuilder_.mergeFrom(value);
          } else {
            valueStateCallBuilder_.setMessage(value);
          }
        }
        methodCase_ = 3;
        return this;
      }
      /**
       * <code>.org.apache.spark.sql.execution.streaming.state.ValueStateCall valueStateCall = 3;</code>
       */
      public Builder clearValueStateCall() {
        if (valueStateCallBuilder_ == null) {
          if (methodCase_ == 3) {
            methodCase_ = 0;
            method_ = null;
            onChanged();
          }
        } else {
          if (methodCase_ == 3) {
            methodCase_ = 0;
            method_ = null;
          }
          valueStateCallBuilder_.clear();
        }
        return this;
      }
      /**
       * <code>.org.apache.spark.sql.execution.streaming.state.ValueStateCall valueStateCall = 3;</code>
       */
      public org.apache.spark.sql.execution.streaming.state.StateMessage.ValueStateCall.Builder getValueStateCallBuilder() {
        return getValueStateCallFieldBuilder().getBuilder();
      }
      /**
       * <code>.org.apache.spark.sql.execution.streaming.state.ValueStateCall valueStateCall = 3;</code>
       */
      @java.lang.Override
      public org.apache.spark.sql.execution.streaming.state.StateMessage.ValueStateCallOrBuilder getValueStateCallOrBuilder() {
        if ((methodCase_ == 3) && (valueStateCallBuilder_ != null)) {
          return valueStateCallBuilder_.getMessageOrBuilder();
        } else {
          if (methodCase_ == 3) {
            return (org.apache.spark.sql.execution.streaming.state.StateMessage.ValueStateCall) method_;
          }
          return org.apache.spark.sql.execution.streaming.state.StateMessage.ValueStateCall.getDefaultInstance();
        }
      }
      /**
       * <code>.org.apache.spark.sql.execution.streaming.state.ValueStateCall valueStateCall = 3;</code>
       */
      private com.google.protobuf.SingleFieldBuilderV3<
          org.apache.spark.sql.execution.streaming.state.StateMessage.ValueStateCall, org.apache.spark.sql.execution.streaming.state.StateMessage.ValueStateCall.Builder, org.apache.spark.sql.execution.streaming.state.StateMessage.ValueStateCallOrBuilder> 
          getValueStateCallFieldBuilder() {
        if (valueStateCallBuilder_ == null) {
          if (!(methodCase_ == 3)) {
            method_ = org.apache.spark.sql.execution.streaming.state.StateMessage.ValueStateCall.getDefaultInstance();
          }
          valueStateCallBuilder_ = new com.google.protobuf.SingleFieldBuilderV3<
              org.apache.spark.sql.execution.streaming.state.StateMessage.ValueStateCall, org.apache.spark.sql.execution.streaming.state.StateMessage.ValueStateCall.Builder, org.apache.spark.sql.execution.streaming.state.StateMessage.ValueStateCallOrBuilder>(
                  (org.apache.spark.sql.execution.streaming.state.StateMessage.ValueStateCall) method_,
                  getParentForChildren(),
                  isClean());
          method_ = null;
        }
        methodCase_ = 3;
        onChanged();
        return valueStateCallBuilder_;
      }
      @java.lang.Override
      public final Builder setUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.setUnknownFields(unknownFields);
      }

      @java.lang.Override
      public final Builder mergeUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.mergeUnknownFields(unknownFields);
      }


      // @@protoc_insertion_point(builder_scope:org.apache.spark.sql.execution.streaming.state.StateRequest)
    }

    // @@protoc_insertion_point(class_scope:org.apache.spark.sql.execution.streaming.state.StateRequest)
    private static final org.apache.spark.sql.execution.streaming.state.StateMessage.StateRequest DEFAULT_INSTANCE;
    static {
      DEFAULT_INSTANCE = new org.apache.spark.sql.execution.streaming.state.StateMessage.StateRequest();
    }

    public static org.apache.spark.sql.execution.streaming.state.StateMessage.StateRequest getDefaultInstance() {
      return DEFAULT_INSTANCE;
    }

    private static final com.google.protobuf.Parser<StateRequest>
        PARSER = new com.google.protobuf.AbstractParser<StateRequest>() {
      @java.lang.Override
      public StateRequest parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        Builder builder = newBuilder();
        try {
          builder.mergeFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          throw e.setUnfinishedMessage(builder.buildPartial());
        } catch (com.google.protobuf.UninitializedMessageException e) {
          throw e.asInvalidProtocolBufferException().setUnfinishedMessage(builder.buildPartial());
        } catch (java.io.IOException e) {
          throw new com.google.protobuf.InvalidProtocolBufferException(e)
              .setUnfinishedMessage(builder.buildPartial());
        }
        return builder.buildPartial();
      }
    };

    public static com.google.protobuf.Parser<StateRequest> parser() {
      return PARSER;
    }

    @java.lang.Override
    public com.google.protobuf.Parser<StateRequest> getParserForType() {
      return PARSER;
    }

    @java.lang.Override
    public org.apache.spark.sql.execution.streaming.state.StateMessage.StateRequest getDefaultInstanceForType() {
      return DEFAULT_INSTANCE;
    }

  }

  public interface StatefulProcessorHandleCallOrBuilder extends
      // @@protoc_insertion_point(interface_extends:org.apache.spark.sql.execution.streaming.state.StatefulProcessorHandleCall)
      com.google.protobuf.MessageOrBuilder {

    /**
     * <code>.org.apache.spark.sql.execution.streaming.state.SetHandleState setHandleState = 2;</code>
     * @return Whether the setHandleState field is set.
     */
    boolean hasSetHandleState();
    /**
     * <code>.org.apache.spark.sql.execution.streaming.state.SetHandleState setHandleState = 2;</code>
     * @return The setHandleState.
     */
    org.apache.spark.sql.execution.streaming.state.StateMessage.SetHandleState getSetHandleState();
    /**
     * <code>.org.apache.spark.sql.execution.streaming.state.SetHandleState setHandleState = 2;</code>
     */
    org.apache.spark.sql.execution.streaming.state.StateMessage.SetHandleStateOrBuilder getSetHandleStateOrBuilder();

    /**
     * <code>.org.apache.spark.sql.execution.streaming.state.GetValueState getValueState = 3;</code>
     * @return Whether the getValueState field is set.
     */
    boolean hasGetValueState();
    /**
     * <code>.org.apache.spark.sql.execution.streaming.state.GetValueState getValueState = 3;</code>
     * @return The getValueState.
     */
    org.apache.spark.sql.execution.streaming.state.StateMessage.GetValueState getGetValueState();
    /**
     * <code>.org.apache.spark.sql.execution.streaming.state.GetValueState getValueState = 3;</code>
     */
    org.apache.spark.sql.execution.streaming.state.StateMessage.GetValueStateOrBuilder getGetValueStateOrBuilder();

    org.apache.spark.sql.execution.streaming.state.StateMessage.StatefulProcessorHandleCall.MethodCase getMethodCase();
  }
  /**
   * Protobuf type {@code org.apache.spark.sql.execution.streaming.state.StatefulProcessorHandleCall}
   */
  public static final class StatefulProcessorHandleCall extends
      com.google.protobuf.GeneratedMessageV3 implements
      // @@protoc_insertion_point(message_implements:org.apache.spark.sql.execution.streaming.state.StatefulProcessorHandleCall)
      StatefulProcessorHandleCallOrBuilder {
  private static final long serialVersionUID = 0L;
    // Use StatefulProcessorHandleCall.newBuilder() to construct.
    private StatefulProcessorHandleCall(com.google.protobuf.GeneratedMessageV3.Builder<?> builder) {
      super(builder);
    }
    private StatefulProcessorHandleCall() {
    }

    @java.lang.Override
    @SuppressWarnings({"unused"})
    protected java.lang.Object newInstance(
        UnusedPrivateParameter unused) {
      return new StatefulProcessorHandleCall();
    }

    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.spark.sql.execution.streaming.state.StateMessage.internal_static_org_apache_spark_sql_execution_streaming_state_StatefulProcessorHandleCall_descriptor;
    }

    @java.lang.Override
    protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.spark.sql.execution.streaming.state.StateMessage.internal_static_org_apache_spark_sql_execution_streaming_state_StatefulProcessorHandleCall_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.spark.sql.execution.streaming.state.StateMessage.StatefulProcessorHandleCall.class, org.apache.spark.sql.execution.streaming.state.StateMessage.StatefulProcessorHandleCall.Builder.class);
    }

    private int methodCase_ = 0;
    @SuppressWarnings("serial")
    private java.lang.Object method_;
    public enum MethodCase
        implements com.google.protobuf.Internal.EnumLite,
            com.google.protobuf.AbstractMessage.InternalOneOfEnum {
      SETHANDLESTATE(2),
      GETVALUESTATE(3),
      METHOD_NOT_SET(0);
      private final int value;
      private MethodCase(int value) {
        this.value = value;
      }
      /**
       * @param value The number of the enum to look for.
       * @return The enum associated with the given number.
       * @deprecated Use {@link #forNumber(int)} instead.
       */
      @java.lang.Deprecated
      public static MethodCase valueOf(int value) {
        return forNumber(value);
      }

      public static MethodCase forNumber(int value) {
        switch (value) {
          case 2: return SETHANDLESTATE;
          case 3: return GETVALUESTATE;
          case 0: return METHOD_NOT_SET;
          default: return null;
        }
      }
      public int getNumber() {
        return this.value;
      }
    };

    public MethodCase
    getMethodCase() {
      return MethodCase.forNumber(
          methodCase_);
    }

    public static final int SETHANDLESTATE_FIELD_NUMBER = 2;
    /**
     * <code>.org.apache.spark.sql.execution.streaming.state.SetHandleState setHandleState = 2;</code>
     * @return Whether the setHandleState field is set.
     */
    @java.lang.Override
    public boolean hasSetHandleState() {
      return methodCase_ == 2;
    }
    /**
     * <code>.org.apache.spark.sql.execution.streaming.state.SetHandleState setHandleState = 2;</code>
     * @return The setHandleState.
     */
    @java.lang.Override
    public org.apache.spark.sql.execution.streaming.state.StateMessage.SetHandleState getSetHandleState() {
      if (methodCase_ == 2) {
         return (org.apache.spark.sql.execution.streaming.state.StateMessage.SetHandleState) method_;
      }
      return org.apache.spark.sql.execution.streaming.state.StateMessage.SetHandleState.getDefaultInstance();
    }
    /**
     * <code>.org.apache.spark.sql.execution.streaming.state.SetHandleState setHandleState = 2;</code>
     */
    @java.lang.Override
    public org.apache.spark.sql.execution.streaming.state.StateMessage.SetHandleStateOrBuilder getSetHandleStateOrBuilder() {
      if (methodCase_ == 2) {
         return (org.apache.spark.sql.execution.streaming.state.StateMessage.SetHandleState) method_;
      }
      return org.apache.spark.sql.execution.streaming.state.StateMessage.SetHandleState.getDefaultInstance();
    }

    public static final int GETVALUESTATE_FIELD_NUMBER = 3;
    /**
     * <code>.org.apache.spark.sql.execution.streaming.state.GetValueState getValueState = 3;</code>
     * @return Whether the getValueState field is set.
     */
    @java.lang.Override
    public boolean hasGetValueState() {
      return methodCase_ == 3;
    }
    /**
     * <code>.org.apache.spark.sql.execution.streaming.state.GetValueState getValueState = 3;</code>
     * @return The getValueState.
     */
    @java.lang.Override
    public org.apache.spark.sql.execution.streaming.state.StateMessage.GetValueState getGetValueState() {
      if (methodCase_ == 3) {
         return (org.apache.spark.sql.execution.streaming.state.StateMessage.GetValueState) method_;
      }
      return org.apache.spark.sql.execution.streaming.state.StateMessage.GetValueState.getDefaultInstance();
    }
    /**
     * <code>.org.apache.spark.sql.execution.streaming.state.GetValueState getValueState = 3;</code>
     */
    @java.lang.Override
    public org.apache.spark.sql.execution.streaming.state.StateMessage.GetValueStateOrBuilder getGetValueStateOrBuilder() {
      if (methodCase_ == 3) {
         return (org.apache.spark.sql.execution.streaming.state.StateMessage.GetValueState) method_;
      }
      return org.apache.spark.sql.execution.streaming.state.StateMessage.GetValueState.getDefaultInstance();
    }

    private byte memoizedIsInitialized = -1;
    @java.lang.Override
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized == 1) return true;
      if (isInitialized == 0) return false;

      memoizedIsInitialized = 1;
      return true;
    }

    @java.lang.Override
    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      if (methodCase_ == 2) {
        output.writeMessage(2, (org.apache.spark.sql.execution.streaming.state.StateMessage.SetHandleState) method_);
      }
      if (methodCase_ == 3) {
        output.writeMessage(3, (org.apache.spark.sql.execution.streaming.state.StateMessage.GetValueState) method_);
      }
      getUnknownFields().writeTo(output);
    }

    @java.lang.Override
    public int getSerializedSize() {
      int size = memoizedSize;
      if (size != -1) return size;

      size = 0;
      if (methodCase_ == 2) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(2, (org.apache.spark.sql.execution.streaming.state.StateMessage.SetHandleState) method_);
      }
      if (methodCase_ == 3) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(3, (org.apache.spark.sql.execution.streaming.state.StateMessage.GetValueState) method_);
      }
      size += getUnknownFields().getSerializedSize();
      memoizedSize = size;
      return size;
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.spark.sql.execution.streaming.state.StateMessage.StatefulProcessorHandleCall)) {
        return super.equals(obj);
      }
      org.apache.spark.sql.execution.streaming.state.StateMessage.StatefulProcessorHandleCall other = (org.apache.spark.sql.execution.streaming.state.StateMessage.StatefulProcessorHandleCall) obj;

      if (!getMethodCase().equals(other.getMethodCase())) return false;
      switch (methodCase_) {
        case 2:
          if (!getSetHandleState()
              .equals(other.getSetHandleState())) return false;
          break;
        case 3:
          if (!getGetValueState()
              .equals(other.getGetValueState())) return false;
          break;
        case 0:
        default:
      }
      if (!getUnknownFields().equals(other.getUnknownFields())) return false;
      return true;
    }

    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptor().hashCode();
      switch (methodCase_) {
        case 2:
          hash = (37 * hash) + SETHANDLESTATE_FIELD_NUMBER;
          hash = (53 * hash) + getSetHandleState().hashCode();
          break;
        case 3:
          hash = (37 * hash) + GETVALUESTATE_FIELD_NUMBER;
          hash = (53 * hash) + getGetValueState().hashCode();
          break;
        case 0:
        default:
      }
      hash = (29 * hash) + getUnknownFields().hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.spark.sql.execution.streaming.state.StateMessage.StatefulProcessorHandleCall parseFrom(
        java.nio.ByteBuffer data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.spark.sql.execution.streaming.state.StateMessage.StatefulProcessorHandleCall parseFrom(
        java.nio.ByteBuffer data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.spark.sql.execution.streaming.state.StateMessage.StatefulProcessorHandleCall parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.spark.sql.execution.streaming.state.StateMessage.StatefulProcessorHandleCall parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.spark.sql.execution.streaming.state.StateMessage.StatefulProcessorHandleCall parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.spark.sql.execution.streaming.state.StateMessage.StatefulProcessorHandleCall parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.spark.sql.execution.streaming.state.StateMessage.StatefulProcessorHandleCall parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.spark.sql.execution.streaming.state.StateMessage.StatefulProcessorHandleCall parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }

    public static org.apache.spark.sql.execution.streaming.state.StateMessage.StatefulProcessorHandleCall parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input);
    }

    public static org.apache.spark.sql.execution.streaming.state.StateMessage.StatefulProcessorHandleCall parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.spark.sql.execution.streaming.state.StateMessage.StatefulProcessorHandleCall parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.spark.sql.execution.streaming.state.StateMessage.StatefulProcessorHandleCall parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }

    @java.lang.Override
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder() {
      return DEFAULT_INSTANCE.toBuilder();
    }
    public static Builder newBuilder(org.apache.spark.sql.execution.streaming.state.StateMessage.StatefulProcessorHandleCall prototype) {
      return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
    }
    @java.lang.Override
    public Builder toBuilder() {
      return this == DEFAULT_INSTANCE
          ? new Builder() : new Builder().mergeFrom(this);
    }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code org.apache.spark.sql.execution.streaming.state.StatefulProcessorHandleCall}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessageV3.Builder<Builder> implements
        // @@protoc_insertion_point(builder_implements:org.apache.spark.sql.execution.streaming.state.StatefulProcessorHandleCall)
        org.apache.spark.sql.execution.streaming.state.StateMessage.StatefulProcessorHandleCallOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.spark.sql.execution.streaming.state.StateMessage.internal_static_org_apache_spark_sql_execution_streaming_state_StatefulProcessorHandleCall_descriptor;
      }

      @java.lang.Override
      protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.spark.sql.execution.streaming.state.StateMessage.internal_static_org_apache_spark_sql_execution_streaming_state_StatefulProcessorHandleCall_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.spark.sql.execution.streaming.state.StateMessage.StatefulProcessorHandleCall.class, org.apache.spark.sql.execution.streaming.state.StateMessage.StatefulProcessorHandleCall.Builder.class);
      }

      // Construct using org.apache.spark.sql.execution.streaming.state.StateMessage.StatefulProcessorHandleCall.newBuilder()
      private Builder() {

      }

      private Builder(
          com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
        super(parent);

      }
      @java.lang.Override
      public Builder clear() {
        super.clear();
        bitField0_ = 0;
        if (setHandleStateBuilder_ != null) {
          setHandleStateBuilder_.clear();
        }
        if (getValueStateBuilder_ != null) {
          getValueStateBuilder_.clear();
        }
        methodCase_ = 0;
        method_ = null;
        return this;
      }

      @java.lang.Override
      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.spark.sql.execution.streaming.state.StateMessage.internal_static_org_apache_spark_sql_execution_streaming_state_StatefulProcessorHandleCall_descriptor;
      }

      @java.lang.Override
      public org.apache.spark.sql.execution.streaming.state.StateMessage.StatefulProcessorHandleCall getDefaultInstanceForType() {
        return org.apache.spark.sql.execution.streaming.state.StateMessage.StatefulProcessorHandleCall.getDefaultInstance();
      }

      @java.lang.Override
      public org.apache.spark.sql.execution.streaming.state.StateMessage.StatefulProcessorHandleCall build() {
        org.apache.spark.sql.execution.streaming.state.StateMessage.StatefulProcessorHandleCall result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      @java.lang.Override
      public org.apache.spark.sql.execution.streaming.state.StateMessage.StatefulProcessorHandleCall buildPartial() {
        org.apache.spark.sql.execution.streaming.state.StateMessage.StatefulProcessorHandleCall result = new org.apache.spark.sql.execution.streaming.state.StateMessage.StatefulProcessorHandleCall(this);
        if (bitField0_ != 0) { buildPartial0(result); }
        buildPartialOneofs(result);
        onBuilt();
        return result;
      }

      private void buildPartial0(org.apache.spark.sql.execution.streaming.state.StateMessage.StatefulProcessorHandleCall result) {
        int from_bitField0_ = bitField0_;
      }

      private void buildPartialOneofs(org.apache.spark.sql.execution.streaming.state.StateMessage.StatefulProcessorHandleCall result) {
        result.methodCase_ = methodCase_;
        result.method_ = this.method_;
        if (methodCase_ == 2 &&
            setHandleStateBuilder_ != null) {
          result.method_ = setHandleStateBuilder_.build();
        }
        if (methodCase_ == 3 &&
            getValueStateBuilder_ != null) {
          result.method_ = getValueStateBuilder_.build();
        }
      }

      @java.lang.Override
      public Builder clone() {
        return super.clone();
      }
      @java.lang.Override
      public Builder setField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.setField(field, value);
      }
      @java.lang.Override
      public Builder clearField(
          com.google.protobuf.Descriptors.FieldDescriptor field) {
        return super.clearField(field);
      }
      @java.lang.Override
      public Builder clearOneof(
          com.google.protobuf.Descriptors.OneofDescriptor oneof) {
        return super.clearOneof(oneof);
      }
      @java.lang.Override
      public Builder setRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          int index, java.lang.Object value) {
        return super.setRepeatedField(field, index, value);
      }
      @java.lang.Override
      public Builder addRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.addRepeatedField(field, value);
      }
      @java.lang.Override
      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.spark.sql.execution.streaming.state.StateMessage.StatefulProcessorHandleCall) {
          return mergeFrom((org.apache.spark.sql.execution.streaming.state.StateMessage.StatefulProcessorHandleCall)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.spark.sql.execution.streaming.state.StateMessage.StatefulProcessorHandleCall other) {
        if (other == org.apache.spark.sql.execution.streaming.state.StateMessage.StatefulProcessorHandleCall.getDefaultInstance()) return this;
        switch (other.getMethodCase()) {
          case SETHANDLESTATE: {
            mergeSetHandleState(other.getSetHandleState());
            break;
          }
          case GETVALUESTATE: {
            mergeGetValueState(other.getGetValueState());
            break;
          }
          case METHOD_NOT_SET: {
            break;
          }
        }
        this.mergeUnknownFields(other.getUnknownFields());
        onChanged();
        return this;
      }

      @java.lang.Override
      public final boolean isInitialized() {
        return true;
      }

      @java.lang.Override
      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        if (extensionRegistry == null) {
          throw new java.lang.NullPointerException();
        }
        try {
          boolean done = false;
          while (!done) {
            int tag = input.readTag();
            switch (tag) {
              case 0:
                done = true;
                break;
              case 18: {
                input.readMessage(
                    getSetHandleStateFieldBuilder().getBuilder(),
                    extensionRegistry);
                methodCase_ = 2;
                break;
              } // case 18
              case 26: {
                input.readMessage(
                    getGetValueStateFieldBuilder().getBuilder(),
                    extensionRegistry);
                methodCase_ = 3;
                break;
              } // case 26
              default: {
                if (!super.parseUnknownField(input, extensionRegistry, tag)) {
                  done = true; // was an endgroup tag
                }
                break;
              } // default:
            } // switch (tag)
          } // while (!done)
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          throw e.unwrapIOException();
        } finally {
          onChanged();
        } // finally
        return this;
      }
      private int methodCase_ = 0;
      private java.lang.Object method_;
      public MethodCase
          getMethodCase() {
        return MethodCase.forNumber(
            methodCase_);
      }

      public Builder clearMethod() {
        methodCase_ = 0;
        method_ = null;
        onChanged();
        return this;
      }

      private int bitField0_;

      private com.google.protobuf.SingleFieldBuilderV3<
          org.apache.spark.sql.execution.streaming.state.StateMessage.SetHandleState, org.apache.spark.sql.execution.streaming.state.StateMessage.SetHandleState.Builder, org.apache.spark.sql.execution.streaming.state.StateMessage.SetHandleStateOrBuilder> setHandleStateBuilder_;
      /**
       * <code>.org.apache.spark.sql.execution.streaming.state.SetHandleState setHandleState = 2;</code>
       * @return Whether the setHandleState field is set.
       */
      @java.lang.Override
      public boolean hasSetHandleState() {
        return methodCase_ == 2;
      }
      /**
       * <code>.org.apache.spark.sql.execution.streaming.state.SetHandleState setHandleState = 2;</code>
       * @return The setHandleState.
       */
      @java.lang.Override
      public org.apache.spark.sql.execution.streaming.state.StateMessage.SetHandleState getSetHandleState() {
        if (setHandleStateBuilder_ == null) {
          if (methodCase_ == 2) {
            return (org.apache.spark.sql.execution.streaming.state.StateMessage.SetHandleState) method_;
          }
          return org.apache.spark.sql.execution.streaming.state.StateMessage.SetHandleState.getDefaultInstance();
        } else {
          if (methodCase_ == 2) {
            return setHandleStateBuilder_.getMessage();
          }
          return org.apache.spark.sql.execution.streaming.state.StateMessage.SetHandleState.getDefaultInstance();
        }
      }
      /**
       * <code>.org.apache.spark.sql.execution.streaming.state.SetHandleState setHandleState = 2;</code>
       */
      public Builder setSetHandleState(org.apache.spark.sql.execution.streaming.state.StateMessage.SetHandleState value) {
        if (setHandleStateBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          method_ = value;
          onChanged();
        } else {
          setHandleStateBuilder_.setMessage(value);
        }
        methodCase_ = 2;
        return this;
      }
      /**
       * <code>.org.apache.spark.sql.execution.streaming.state.SetHandleState setHandleState = 2;</code>
       */
      public Builder setSetHandleState(
          org.apache.spark.sql.execution.streaming.state.StateMessage.SetHandleState.Builder builderForValue) {
        if (setHandleStateBuilder_ == null) {
          method_ = builderForValue.build();
          onChanged();
        } else {
          setHandleStateBuilder_.setMessage(builderForValue.build());
        }
        methodCase_ = 2;
        return this;
      }
      /**
       * <code>.org.apache.spark.sql.execution.streaming.state.SetHandleState setHandleState = 2;</code>
       */
      public Builder mergeSetHandleState(org.apache.spark.sql.execution.streaming.state.StateMessage.SetHandleState value) {
        if (setHandleStateBuilder_ == null) {
          if (methodCase_ == 2 &&
              method_ != org.apache.spark.sql.execution.streaming.state.StateMessage.SetHandleState.getDefaultInstance()) {
            method_ = org.apache.spark.sql.execution.streaming.state.StateMessage.SetHandleState.newBuilder((org.apache.spark.sql.execution.streaming.state.StateMessage.SetHandleState) method_)
                .mergeFrom(value).buildPartial();
          } else {
            method_ = value;
          }
          onChanged();
        } else {
          if (methodCase_ == 2) {
            setHandleStateBuilder_.mergeFrom(value);
          } else {
            setHandleStateBuilder_.setMessage(value);
          }
        }
        methodCase_ = 2;
        return this;
      }
      /**
       * <code>.org.apache.spark.sql.execution.streaming.state.SetHandleState setHandleState = 2;</code>
       */
      public Builder clearSetHandleState() {
        if (setHandleStateBuilder_ == null) {
          if (methodCase_ == 2) {
            methodCase_ = 0;
            method_ = null;
            onChanged();
          }
        } else {
          if (methodCase_ == 2) {
            methodCase_ = 0;
            method_ = null;
          }
          setHandleStateBuilder_.clear();
        }
        return this;
      }
      /**
       * <code>.org.apache.spark.sql.execution.streaming.state.SetHandleState setHandleState = 2;</code>
       */
      public org.apache.spark.sql.execution.streaming.state.StateMessage.SetHandleState.Builder getSetHandleStateBuilder() {
        return getSetHandleStateFieldBuilder().getBuilder();
      }
      /**
       * <code>.org.apache.spark.sql.execution.streaming.state.SetHandleState setHandleState = 2;</code>
       */
      @java.lang.Override
      public org.apache.spark.sql.execution.streaming.state.StateMessage.SetHandleStateOrBuilder getSetHandleStateOrBuilder() {
        if ((methodCase_ == 2) && (setHandleStateBuilder_ != null)) {
          return setHandleStateBuilder_.getMessageOrBuilder();
        } else {
          if (methodCase_ == 2) {
            return (org.apache.spark.sql.execution.streaming.state.StateMessage.SetHandleState) method_;
          }
          return org.apache.spark.sql.execution.streaming.state.StateMessage.SetHandleState.getDefaultInstance();
        }
      }
      /**
       * <code>.org.apache.spark.sql.execution.streaming.state.SetHandleState setHandleState = 2;</code>
       */
      private com.google.protobuf.SingleFieldBuilderV3<
          org.apache.spark.sql.execution.streaming.state.StateMessage.SetHandleState, org.apache.spark.sql.execution.streaming.state.StateMessage.SetHandleState.Builder, org.apache.spark.sql.execution.streaming.state.StateMessage.SetHandleStateOrBuilder> 
          getSetHandleStateFieldBuilder() {
        if (setHandleStateBuilder_ == null) {
          if (!(methodCase_ == 2)) {
            method_ = org.apache.spark.sql.execution.streaming.state.StateMessage.SetHandleState.getDefaultInstance();
          }
          setHandleStateBuilder_ = new com.google.protobuf.SingleFieldBuilderV3<
              org.apache.spark.sql.execution.streaming.state.StateMessage.SetHandleState, org.apache.spark.sql.execution.streaming.state.StateMessage.SetHandleState.Builder, org.apache.spark.sql.execution.streaming.state.StateMessage.SetHandleStateOrBuilder>(
                  (org.apache.spark.sql.execution.streaming.state.StateMessage.SetHandleState) method_,
                  getParentForChildren(),
                  isClean());
          method_ = null;
        }
        methodCase_ = 2;
        onChanged();
        return setHandleStateBuilder_;
      }

      private com.google.protobuf.SingleFieldBuilderV3<
          org.apache.spark.sql.execution.streaming.state.StateMessage.GetValueState, org.apache.spark.sql.execution.streaming.state.StateMessage.GetValueState.Builder, org.apache.spark.sql.execution.streaming.state.StateMessage.GetValueStateOrBuilder> getValueStateBuilder_;
      /**
       * <code>.org.apache.spark.sql.execution.streaming.state.GetValueState getValueState = 3;</code>
       * @return Whether the getValueState field is set.
       */
      @java.lang.Override
      public boolean hasGetValueState() {
        return methodCase_ == 3;
      }
      /**
       * <code>.org.apache.spark.sql.execution.streaming.state.GetValueState getValueState = 3;</code>
       * @return The getValueState.
       */
      @java.lang.Override
      public org.apache.spark.sql.execution.streaming.state.StateMessage.GetValueState getGetValueState() {
        if (getValueStateBuilder_ == null) {
          if (methodCase_ == 3) {
            return (org.apache.spark.sql.execution.streaming.state.StateMessage.GetValueState) method_;
          }
          return org.apache.spark.sql.execution.streaming.state.StateMessage.GetValueState.getDefaultInstance();
        } else {
          if (methodCase_ == 3) {
            return getValueStateBuilder_.getMessage();
          }
          return org.apache.spark.sql.execution.streaming.state.StateMessage.GetValueState.getDefaultInstance();
        }
      }
      /**
       * <code>.org.apache.spark.sql.execution.streaming.state.GetValueState getValueState = 3;</code>
       */
      public Builder setGetValueState(org.apache.spark.sql.execution.streaming.state.StateMessage.GetValueState value) {
        if (getValueStateBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          method_ = value;
          onChanged();
        } else {
          getValueStateBuilder_.setMessage(value);
        }
        methodCase_ = 3;
        return this;
      }
      /**
       * <code>.org.apache.spark.sql.execution.streaming.state.GetValueState getValueState = 3;</code>
       */
      public Builder setGetValueState(
          org.apache.spark.sql.execution.streaming.state.StateMessage.GetValueState.Builder builderForValue) {
        if (getValueStateBuilder_ == null) {
          method_ = builderForValue.build();
          onChanged();
        } else {
          getValueStateBuilder_.setMessage(builderForValue.build());
        }
        methodCase_ = 3;
        return this;
      }
      /**
       * <code>.org.apache.spark.sql.execution.streaming.state.GetValueState getValueState = 3;</code>
       */
      public Builder mergeGetValueState(org.apache.spark.sql.execution.streaming.state.StateMessage.GetValueState value) {
        if (getValueStateBuilder_ == null) {
          if (methodCase_ == 3 &&
              method_ != org.apache.spark.sql.execution.streaming.state.StateMessage.GetValueState.getDefaultInstance()) {
            method_ = org.apache.spark.sql.execution.streaming.state.StateMessage.GetValueState.newBuilder((org.apache.spark.sql.execution.streaming.state.StateMessage.GetValueState) method_)
                .mergeFrom(value).buildPartial();
          } else {
            method_ = value;
          }
          onChanged();
        } else {
          if (methodCase_ == 3) {
            getValueStateBuilder_.mergeFrom(value);
          } else {
            getValueStateBuilder_.setMessage(value);
          }
        }
        methodCase_ = 3;
        return this;
      }
      /**
       * <code>.org.apache.spark.sql.execution.streaming.state.GetValueState getValueState = 3;</code>
       */
      public Builder clearGetValueState() {
        if (getValueStateBuilder_ == null) {
          if (methodCase_ == 3) {
            methodCase_ = 0;
            method_ = null;
            onChanged();
          }
        } else {
          if (methodCase_ == 3) {
            methodCase_ = 0;
            method_ = null;
          }
          getValueStateBuilder_.clear();
        }
        return this;
      }
      /**
       * <code>.org.apache.spark.sql.execution.streaming.state.GetValueState getValueState = 3;</code>
       */
      public org.apache.spark.sql.execution.streaming.state.StateMessage.GetValueState.Builder getGetValueStateBuilder() {
        return getGetValueStateFieldBuilder().getBuilder();
      }
      /**
       * <code>.org.apache.spark.sql.execution.streaming.state.GetValueState getValueState = 3;</code>
       */
      @java.lang.Override
      public org.apache.spark.sql.execution.streaming.state.StateMessage.GetValueStateOrBuilder getGetValueStateOrBuilder() {
        if ((methodCase_ == 3) && (getValueStateBuilder_ != null)) {
          return getValueStateBuilder_.getMessageOrBuilder();
        } else {
          if (methodCase_ == 3) {
            return (org.apache.spark.sql.execution.streaming.state.StateMessage.GetValueState) method_;
          }
          return org.apache.spark.sql.execution.streaming.state.StateMessage.GetValueState.getDefaultInstance();
        }
      }
      /**
       * <code>.org.apache.spark.sql.execution.streaming.state.GetValueState getValueState = 3;</code>
       */
      private com.google.protobuf.SingleFieldBuilderV3<
          org.apache.spark.sql.execution.streaming.state.StateMessage.GetValueState, org.apache.spark.sql.execution.streaming.state.StateMessage.GetValueState.Builder, org.apache.spark.sql.execution.streaming.state.StateMessage.GetValueStateOrBuilder> 
          getGetValueStateFieldBuilder() {
        if (getValueStateBuilder_ == null) {
          if (!(methodCase_ == 3)) {
            method_ = org.apache.spark.sql.execution.streaming.state.StateMessage.GetValueState.getDefaultInstance();
          }
          getValueStateBuilder_ = new com.google.protobuf.SingleFieldBuilderV3<
              org.apache.spark.sql.execution.streaming.state.StateMessage.GetValueState, org.apache.spark.sql.execution.streaming.state.StateMessage.GetValueState.Builder, org.apache.spark.sql.execution.streaming.state.StateMessage.GetValueStateOrBuilder>(
                  (org.apache.spark.sql.execution.streaming.state.StateMessage.GetValueState) method_,
                  getParentForChildren(),
                  isClean());
          method_ = null;
        }
        methodCase_ = 3;
        onChanged();
        return getValueStateBuilder_;
      }
      @java.lang.Override
      public final Builder setUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.setUnknownFields(unknownFields);
      }

      @java.lang.Override
      public final Builder mergeUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.mergeUnknownFields(unknownFields);
      }


      // @@protoc_insertion_point(builder_scope:org.apache.spark.sql.execution.streaming.state.StatefulProcessorHandleCall)
    }

    // @@protoc_insertion_point(class_scope:org.apache.spark.sql.execution.streaming.state.StatefulProcessorHandleCall)
    private static final org.apache.spark.sql.execution.streaming.state.StateMessage.StatefulProcessorHandleCall DEFAULT_INSTANCE;
    static {
      DEFAULT_INSTANCE = new org.apache.spark.sql.execution.streaming.state.StateMessage.StatefulProcessorHandleCall();
    }

    public static org.apache.spark.sql.execution.streaming.state.StateMessage.StatefulProcessorHandleCall getDefaultInstance() {
      return DEFAULT_INSTANCE;
    }

    private static final com.google.protobuf.Parser<StatefulProcessorHandleCall>
        PARSER = new com.google.protobuf.AbstractParser<StatefulProcessorHandleCall>() {
      @java.lang.Override
      public StatefulProcessorHandleCall parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        Builder builder = newBuilder();
        try {
          builder.mergeFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          throw e.setUnfinishedMessage(builder.buildPartial());
        } catch (com.google.protobuf.UninitializedMessageException e) {
          throw e.asInvalidProtocolBufferException().setUnfinishedMessage(builder.buildPartial());
        } catch (java.io.IOException e) {
          throw new com.google.protobuf.InvalidProtocolBufferException(e)
              .setUnfinishedMessage(builder.buildPartial());
        }
        return builder.buildPartial();
      }
    };

    public static com.google.protobuf.Parser<StatefulProcessorHandleCall> parser() {
      return PARSER;
    }

    @java.lang.Override
    public com.google.protobuf.Parser<StatefulProcessorHandleCall> getParserForType() {
      return PARSER;
    }

    @java.lang.Override
    public org.apache.spark.sql.execution.streaming.state.StateMessage.StatefulProcessorHandleCall getDefaultInstanceForType() {
      return DEFAULT_INSTANCE;
    }

  }

  public interface GetValueStateOrBuilder extends
      // @@protoc_insertion_point(interface_extends:org.apache.spark.sql.execution.streaming.state.GetValueState)
      com.google.protobuf.MessageOrBuilder {

    /**
     * <code>string stateName = 1;</code>
     * @return The stateName.
     */
    java.lang.String getStateName();
    /**
     * <code>string stateName = 1;</code>
     * @return The bytes for stateName.
     */
    com.google.protobuf.ByteString
        getStateNameBytes();
  }
  /**
   * Protobuf type {@code org.apache.spark.sql.execution.streaming.state.GetValueState}
   */
  public static final class GetValueState extends
      com.google.protobuf.GeneratedMessageV3 implements
      // @@protoc_insertion_point(message_implements:org.apache.spark.sql.execution.streaming.state.GetValueState)
      GetValueStateOrBuilder {
  private static final long serialVersionUID = 0L;
    // Use GetValueState.newBuilder() to construct.
    private GetValueState(com.google.protobuf.GeneratedMessageV3.Builder<?> builder) {
      super(builder);
    }
    private GetValueState() {
      stateName_ = "";
    }

    @java.lang.Override
    @SuppressWarnings({"unused"})
    protected java.lang.Object newInstance(
        UnusedPrivateParameter unused) {
      return new GetValueState();
    }

    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.spark.sql.execution.streaming.state.StateMessage.internal_static_org_apache_spark_sql_execution_streaming_state_GetValueState_descriptor;
    }

    @java.lang.Override
    protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.spark.sql.execution.streaming.state.StateMessage.internal_static_org_apache_spark_sql_execution_streaming_state_GetValueState_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.spark.sql.execution.streaming.state.StateMessage.GetValueState.class, org.apache.spark.sql.execution.streaming.state.StateMessage.GetValueState.Builder.class);
    }

    public static final int STATENAME_FIELD_NUMBER = 1;
    @SuppressWarnings("serial")
    private volatile java.lang.Object stateName_ = "";
    /**
     * <code>string stateName = 1;</code>
     * @return The stateName.
     */
    @java.lang.Override
    public java.lang.String getStateName() {
      java.lang.Object ref = stateName_;
      if (ref instanceof java.lang.String) {
        return (java.lang.String) ref;
      } else {
        com.google.protobuf.ByteString bs = 
            (com.google.protobuf.ByteString) ref;
        java.lang.String s = bs.toStringUtf8();
        stateName_ = s;
        return s;
      }
    }
    /**
     * <code>string stateName = 1;</code>
     * @return The bytes for stateName.
     */
    @java.lang.Override
    public com.google.protobuf.ByteString
        getStateNameBytes() {
      java.lang.Object ref = stateName_;
      if (ref instanceof java.lang.String) {
        com.google.protobuf.ByteString b = 
            com.google.protobuf.ByteString.copyFromUtf8(
                (java.lang.String) ref);
        stateName_ = b;
        return b;
      } else {
        return (com.google.protobuf.ByteString) ref;
      }
    }

    private byte memoizedIsInitialized = -1;
    @java.lang.Override
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized == 1) return true;
      if (isInitialized == 0) return false;

      memoizedIsInitialized = 1;
      return true;
    }

    @java.lang.Override
    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      if (!com.google.protobuf.GeneratedMessageV3.isStringEmpty(stateName_)) {
        com.google.protobuf.GeneratedMessageV3.writeString(output, 1, stateName_);
      }
      getUnknownFields().writeTo(output);
    }

    @java.lang.Override
    public int getSerializedSize() {
      int size = memoizedSize;
      if (size != -1) return size;

      size = 0;
      if (!com.google.protobuf.GeneratedMessageV3.isStringEmpty(stateName_)) {
        size += com.google.protobuf.GeneratedMessageV3.computeStringSize(1, stateName_);
      }
      size += getUnknownFields().getSerializedSize();
      memoizedSize = size;
      return size;
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.spark.sql.execution.streaming.state.StateMessage.GetValueState)) {
        return super.equals(obj);
      }
      org.apache.spark.sql.execution.streaming.state.StateMessage.GetValueState other = (org.apache.spark.sql.execution.streaming.state.StateMessage.GetValueState) obj;

      if (!getStateName()
          .equals(other.getStateName())) return false;
      if (!getUnknownFields().equals(other.getUnknownFields())) return false;
      return true;
    }

    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptor().hashCode();
      hash = (37 * hash) + STATENAME_FIELD_NUMBER;
      hash = (53 * hash) + getStateName().hashCode();
      hash = (29 * hash) + getUnknownFields().hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.spark.sql.execution.streaming.state.StateMessage.GetValueState parseFrom(
        java.nio.ByteBuffer data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.spark.sql.execution.streaming.state.StateMessage.GetValueState parseFrom(
        java.nio.ByteBuffer data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.spark.sql.execution.streaming.state.StateMessage.GetValueState parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.spark.sql.execution.streaming.state.StateMessage.GetValueState parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.spark.sql.execution.streaming.state.StateMessage.GetValueState parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.spark.sql.execution.streaming.state.StateMessage.GetValueState parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.spark.sql.execution.streaming.state.StateMessage.GetValueState parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.spark.sql.execution.streaming.state.StateMessage.GetValueState parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }

    public static org.apache.spark.sql.execution.streaming.state.StateMessage.GetValueState parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input);
    }

    public static org.apache.spark.sql.execution.streaming.state.StateMessage.GetValueState parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.spark.sql.execution.streaming.state.StateMessage.GetValueState parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.spark.sql.execution.streaming.state.StateMessage.GetValueState parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }

    @java.lang.Override
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder() {
      return DEFAULT_INSTANCE.toBuilder();
    }
    public static Builder newBuilder(org.apache.spark.sql.execution.streaming.state.StateMessage.GetValueState prototype) {
      return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
    }
    @java.lang.Override
    public Builder toBuilder() {
      return this == DEFAULT_INSTANCE
          ? new Builder() : new Builder().mergeFrom(this);
    }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code org.apache.spark.sql.execution.streaming.state.GetValueState}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessageV3.Builder<Builder> implements
        // @@protoc_insertion_point(builder_implements:org.apache.spark.sql.execution.streaming.state.GetValueState)
        org.apache.spark.sql.execution.streaming.state.StateMessage.GetValueStateOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.spark.sql.execution.streaming.state.StateMessage.internal_static_org_apache_spark_sql_execution_streaming_state_GetValueState_descriptor;
      }

      @java.lang.Override
      protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.spark.sql.execution.streaming.state.StateMessage.internal_static_org_apache_spark_sql_execution_streaming_state_GetValueState_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.spark.sql.execution.streaming.state.StateMessage.GetValueState.class, org.apache.spark.sql.execution.streaming.state.StateMessage.GetValueState.Builder.class);
      }

      // Construct using org.apache.spark.sql.execution.streaming.state.StateMessage.GetValueState.newBuilder()
      private Builder() {

      }

      private Builder(
          com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
        super(parent);

      }
      @java.lang.Override
      public Builder clear() {
        super.clear();
        bitField0_ = 0;
        stateName_ = "";
        return this;
      }

      @java.lang.Override
      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.spark.sql.execution.streaming.state.StateMessage.internal_static_org_apache_spark_sql_execution_streaming_state_GetValueState_descriptor;
      }

      @java.lang.Override
      public org.apache.spark.sql.execution.streaming.state.StateMessage.GetValueState getDefaultInstanceForType() {
        return org.apache.spark.sql.execution.streaming.state.StateMessage.GetValueState.getDefaultInstance();
      }

      @java.lang.Override
      public org.apache.spark.sql.execution.streaming.state.StateMessage.GetValueState build() {
        org.apache.spark.sql.execution.streaming.state.StateMessage.GetValueState result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      @java.lang.Override
      public org.apache.spark.sql.execution.streaming.state.StateMessage.GetValueState buildPartial() {
        org.apache.spark.sql.execution.streaming.state.StateMessage.GetValueState result = new org.apache.spark.sql.execution.streaming.state.StateMessage.GetValueState(this);
        if (bitField0_ != 0) { buildPartial0(result); }
        onBuilt();
        return result;
      }

      private void buildPartial0(org.apache.spark.sql.execution.streaming.state.StateMessage.GetValueState result) {
        int from_bitField0_ = bitField0_;
        if (((from_bitField0_ & 0x00000001) != 0)) {
          result.stateName_ = stateName_;
        }
      }

      @java.lang.Override
      public Builder clone() {
        return super.clone();
      }
      @java.lang.Override
      public Builder setField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.setField(field, value);
      }
      @java.lang.Override
      public Builder clearField(
          com.google.protobuf.Descriptors.FieldDescriptor field) {
        return super.clearField(field);
      }
      @java.lang.Override
      public Builder clearOneof(
          com.google.protobuf.Descriptors.OneofDescriptor oneof) {
        return super.clearOneof(oneof);
      }
      @java.lang.Override
      public Builder setRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          int index, java.lang.Object value) {
        return super.setRepeatedField(field, index, value);
      }
      @java.lang.Override
      public Builder addRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.addRepeatedField(field, value);
      }
      @java.lang.Override
      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.spark.sql.execution.streaming.state.StateMessage.GetValueState) {
          return mergeFrom((org.apache.spark.sql.execution.streaming.state.StateMessage.GetValueState)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.spark.sql.execution.streaming.state.StateMessage.GetValueState other) {
        if (other == org.apache.spark.sql.execution.streaming.state.StateMessage.GetValueState.getDefaultInstance()) return this;
        if (!other.getStateName().isEmpty()) {
          stateName_ = other.stateName_;
          bitField0_ |= 0x00000001;
          onChanged();
        }
        this.mergeUnknownFields(other.getUnknownFields());
        onChanged();
        return this;
      }

      @java.lang.Override
      public final boolean isInitialized() {
        return true;
      }

      @java.lang.Override
      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        if (extensionRegistry == null) {
          throw new java.lang.NullPointerException();
        }
        try {
          boolean done = false;
          while (!done) {
            int tag = input.readTag();
            switch (tag) {
              case 0:
                done = true;
                break;
              case 10: {
                stateName_ = input.readStringRequireUtf8();
                bitField0_ |= 0x00000001;
                break;
              } // case 10
              default: {
                if (!super.parseUnknownField(input, extensionRegistry, tag)) {
                  done = true; // was an endgroup tag
                }
                break;
              } // default:
            } // switch (tag)
          } // while (!done)
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          throw e.unwrapIOException();
        } finally {
          onChanged();
        } // finally
        return this;
      }
      private int bitField0_;

      private java.lang.Object stateName_ = "";
      /**
       * <code>string stateName = 1;</code>
       * @return The stateName.
       */
      public java.lang.String getStateName() {
        java.lang.Object ref = stateName_;
        if (!(ref instanceof java.lang.String)) {
          com.google.protobuf.ByteString bs =
              (com.google.protobuf.ByteString) ref;
          java.lang.String s = bs.toStringUtf8();
          stateName_ = s;
          return s;
        } else {
          return (java.lang.String) ref;
        }
      }
      /**
       * <code>string stateName = 1;</code>
       * @return The bytes for stateName.
       */
      public com.google.protobuf.ByteString
          getStateNameBytes() {
        java.lang.Object ref = stateName_;
        if (ref instanceof String) {
          com.google.protobuf.ByteString b = 
              com.google.protobuf.ByteString.copyFromUtf8(
                  (java.lang.String) ref);
          stateName_ = b;
          return b;
        } else {
          return (com.google.protobuf.ByteString) ref;
        }
      }
      /**
       * <code>string stateName = 1;</code>
       * @param value The stateName to set.
       * @return This builder for chaining.
       */
      public Builder setStateName(
          java.lang.String value) {
        if (value == null) { throw new NullPointerException(); }
        stateName_ = value;
        bitField0_ |= 0x00000001;
        onChanged();
        return this;
      }
      /**
       * <code>string stateName = 1;</code>
       * @return This builder for chaining.
       */
      public Builder clearStateName() {
        stateName_ = getDefaultInstance().getStateName();
        bitField0_ = (bitField0_ & ~0x00000001);
        onChanged();
        return this;
      }
      /**
       * <code>string stateName = 1;</code>
       * @param value The bytes for stateName to set.
       * @return This builder for chaining.
       */
      public Builder setStateNameBytes(
          com.google.protobuf.ByteString value) {
        if (value == null) { throw new NullPointerException(); }
        checkByteStringIsUtf8(value);
        stateName_ = value;
        bitField0_ |= 0x00000001;
        onChanged();
        return this;
      }
      @java.lang.Override
      public final Builder setUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.setUnknownFields(unknownFields);
      }

      @java.lang.Override
      public final Builder mergeUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.mergeUnknownFields(unknownFields);
      }


      // @@protoc_insertion_point(builder_scope:org.apache.spark.sql.execution.streaming.state.GetValueState)
    }

    // @@protoc_insertion_point(class_scope:org.apache.spark.sql.execution.streaming.state.GetValueState)
    private static final org.apache.spark.sql.execution.streaming.state.StateMessage.GetValueState DEFAULT_INSTANCE;
    static {
      DEFAULT_INSTANCE = new org.apache.spark.sql.execution.streaming.state.StateMessage.GetValueState();
    }

    public static org.apache.spark.sql.execution.streaming.state.StateMessage.GetValueState getDefaultInstance() {
      return DEFAULT_INSTANCE;
    }

    private static final com.google.protobuf.Parser<GetValueState>
        PARSER = new com.google.protobuf.AbstractParser<GetValueState>() {
      @java.lang.Override
      public GetValueState parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        Builder builder = newBuilder();
        try {
          builder.mergeFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          throw e.setUnfinishedMessage(builder.buildPartial());
        } catch (com.google.protobuf.UninitializedMessageException e) {
          throw e.asInvalidProtocolBufferException().setUnfinishedMessage(builder.buildPartial());
        } catch (java.io.IOException e) {
          throw new com.google.protobuf.InvalidProtocolBufferException(e)
              .setUnfinishedMessage(builder.buildPartial());
        }
        return builder.buildPartial();
      }
    };

    public static com.google.protobuf.Parser<GetValueState> parser() {
      return PARSER;
    }

    @java.lang.Override
    public com.google.protobuf.Parser<GetValueState> getParserForType() {
      return PARSER;
    }

    @java.lang.Override
    public org.apache.spark.sql.execution.streaming.state.StateMessage.GetValueState getDefaultInstanceForType() {
      return DEFAULT_INSTANCE;
    }

  }

  public interface ValueStateCallOrBuilder extends
      // @@protoc_insertion_point(interface_extends:org.apache.spark.sql.execution.streaming.state.ValueStateCall)
      com.google.protobuf.MessageOrBuilder {

    /**
     * <code>.org.apache.spark.sql.execution.streaming.state.ValueStateExists exists = 1;</code>
     * @return Whether the exists field is set.
     */
    boolean hasExists();
    /**
     * <code>.org.apache.spark.sql.execution.streaming.state.ValueStateExists exists = 1;</code>
     * @return The exists.
     */
    org.apache.spark.sql.execution.streaming.state.StateMessage.ValueStateExists getExists();
    /**
     * <code>.org.apache.spark.sql.execution.streaming.state.ValueStateExists exists = 1;</code>
     */
    org.apache.spark.sql.execution.streaming.state.StateMessage.ValueStateExistsOrBuilder getExistsOrBuilder();

    /**
     * <code>.org.apache.spark.sql.execution.streaming.state.ValueStateGet get = 2;</code>
     * @return Whether the get field is set.
     */
    boolean hasGet();
    /**
     * <code>.org.apache.spark.sql.execution.streaming.state.ValueStateGet get = 2;</code>
     * @return The get.
     */
    org.apache.spark.sql.execution.streaming.state.StateMessage.ValueStateGet getGet();
    /**
     * <code>.org.apache.spark.sql.execution.streaming.state.ValueStateGet get = 2;</code>
     */
    org.apache.spark.sql.execution.streaming.state.StateMessage.ValueStateGetOrBuilder getGetOrBuilder();

    /**
     * <code>.org.apache.spark.sql.execution.streaming.state.ValueStateUpdate update = 3;</code>
     * @return Whether the update field is set.
     */
    boolean hasUpdate();
    /**
     * <code>.org.apache.spark.sql.execution.streaming.state.ValueStateUpdate update = 3;</code>
     * @return The update.
     */
    org.apache.spark.sql.execution.streaming.state.StateMessage.ValueStateUpdate getUpdate();
    /**
     * <code>.org.apache.spark.sql.execution.streaming.state.ValueStateUpdate update = 3;</code>
     */
    org.apache.spark.sql.execution.streaming.state.StateMessage.ValueStateUpdateOrBuilder getUpdateOrBuilder();

    org.apache.spark.sql.execution.streaming.state.StateMessage.ValueStateCall.MethodCase getMethodCase();
  }
  /**
   * Protobuf type {@code org.apache.spark.sql.execution.streaming.state.ValueStateCall}
   */
  public static final class ValueStateCall extends
      com.google.protobuf.GeneratedMessageV3 implements
      // @@protoc_insertion_point(message_implements:org.apache.spark.sql.execution.streaming.state.ValueStateCall)
      ValueStateCallOrBuilder {
  private static final long serialVersionUID = 0L;
    // Use ValueStateCall.newBuilder() to construct.
    private ValueStateCall(com.google.protobuf.GeneratedMessageV3.Builder<?> builder) {
      super(builder);
    }
    private ValueStateCall() {
    }

    @java.lang.Override
    @SuppressWarnings({"unused"})
    protected java.lang.Object newInstance(
        UnusedPrivateParameter unused) {
      return new ValueStateCall();
    }

    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.spark.sql.execution.streaming.state.StateMessage.internal_static_org_apache_spark_sql_execution_streaming_state_ValueStateCall_descriptor;
    }

    @java.lang.Override
    protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.spark.sql.execution.streaming.state.StateMessage.internal_static_org_apache_spark_sql_execution_streaming_state_ValueStateCall_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.spark.sql.execution.streaming.state.StateMessage.ValueStateCall.class, org.apache.spark.sql.execution.streaming.state.StateMessage.ValueStateCall.Builder.class);
    }

    private int methodCase_ = 0;
    @SuppressWarnings("serial")
    private java.lang.Object method_;
    public enum MethodCase
        implements com.google.protobuf.Internal.EnumLite,
            com.google.protobuf.AbstractMessage.InternalOneOfEnum {
      EXISTS(1),
      GET(2),
      UPDATE(3),
      METHOD_NOT_SET(0);
      private final int value;
      private MethodCase(int value) {
        this.value = value;
      }
      /**
       * @param value The number of the enum to look for.
       * @return The enum associated with the given number.
       * @deprecated Use {@link #forNumber(int)} instead.
       */
      @java.lang.Deprecated
      public static MethodCase valueOf(int value) {
        return forNumber(value);
      }

      public static MethodCase forNumber(int value) {
        switch (value) {
          case 1: return EXISTS;
          case 2: return GET;
          case 3: return UPDATE;
          case 0: return METHOD_NOT_SET;
          default: return null;
        }
      }
      public int getNumber() {
        return this.value;
      }
    };

    public MethodCase
    getMethodCase() {
      return MethodCase.forNumber(
          methodCase_);
    }

    public static final int EXISTS_FIELD_NUMBER = 1;
    /**
     * <code>.org.apache.spark.sql.execution.streaming.state.ValueStateExists exists = 1;</code>
     * @return Whether the exists field is set.
     */
    @java.lang.Override
    public boolean hasExists() {
      return methodCase_ == 1;
    }
    /**
     * <code>.org.apache.spark.sql.execution.streaming.state.ValueStateExists exists = 1;</code>
     * @return The exists.
     */
    @java.lang.Override
    public org.apache.spark.sql.execution.streaming.state.StateMessage.ValueStateExists getExists() {
      if (methodCase_ == 1) {
         return (org.apache.spark.sql.execution.streaming.state.StateMessage.ValueStateExists) method_;
      }
      return org.apache.spark.sql.execution.streaming.state.StateMessage.ValueStateExists.getDefaultInstance();
    }
    /**
     * <code>.org.apache.spark.sql.execution.streaming.state.ValueStateExists exists = 1;</code>
     */
    @java.lang.Override
    public org.apache.spark.sql.execution.streaming.state.StateMessage.ValueStateExistsOrBuilder getExistsOrBuilder() {
      if (methodCase_ == 1) {
         return (org.apache.spark.sql.execution.streaming.state.StateMessage.ValueStateExists) method_;
      }
      return org.apache.spark.sql.execution.streaming.state.StateMessage.ValueStateExists.getDefaultInstance();
    }

    public static final int GET_FIELD_NUMBER = 2;
    /**
     * <code>.org.apache.spark.sql.execution.streaming.state.ValueStateGet get = 2;</code>
     * @return Whether the get field is set.
     */
    @java.lang.Override
    public boolean hasGet() {
      return methodCase_ == 2;
    }
    /**
     * <code>.org.apache.spark.sql.execution.streaming.state.ValueStateGet get = 2;</code>
     * @return The get.
     */
    @java.lang.Override
    public org.apache.spark.sql.execution.streaming.state.StateMessage.ValueStateGet getGet() {
      if (methodCase_ == 2) {
         return (org.apache.spark.sql.execution.streaming.state.StateMessage.ValueStateGet) method_;
      }
      return org.apache.spark.sql.execution.streaming.state.StateMessage.ValueStateGet.getDefaultInstance();
    }
    /**
     * <code>.org.apache.spark.sql.execution.streaming.state.ValueStateGet get = 2;</code>
     */
    @java.lang.Override
    public org.apache.spark.sql.execution.streaming.state.StateMessage.ValueStateGetOrBuilder getGetOrBuilder() {
      if (methodCase_ == 2) {
         return (org.apache.spark.sql.execution.streaming.state.StateMessage.ValueStateGet) method_;
      }
      return org.apache.spark.sql.execution.streaming.state.StateMessage.ValueStateGet.getDefaultInstance();
    }

    public static final int UPDATE_FIELD_NUMBER = 3;
    /**
     * <code>.org.apache.spark.sql.execution.streaming.state.ValueStateUpdate update = 3;</code>
     * @return Whether the update field is set.
     */
    @java.lang.Override
    public boolean hasUpdate() {
      return methodCase_ == 3;
    }
    /**
     * <code>.org.apache.spark.sql.execution.streaming.state.ValueStateUpdate update = 3;</code>
     * @return The update.
     */
    @java.lang.Override
    public org.apache.spark.sql.execution.streaming.state.StateMessage.ValueStateUpdate getUpdate() {
      if (methodCase_ == 3) {
         return (org.apache.spark.sql.execution.streaming.state.StateMessage.ValueStateUpdate) method_;
      }
      return org.apache.spark.sql.execution.streaming.state.StateMessage.ValueStateUpdate.getDefaultInstance();
    }
    /**
     * <code>.org.apache.spark.sql.execution.streaming.state.ValueStateUpdate update = 3;</code>
     */
    @java.lang.Override
    public org.apache.spark.sql.execution.streaming.state.StateMessage.ValueStateUpdateOrBuilder getUpdateOrBuilder() {
      if (methodCase_ == 3) {
         return (org.apache.spark.sql.execution.streaming.state.StateMessage.ValueStateUpdate) method_;
      }
      return org.apache.spark.sql.execution.streaming.state.StateMessage.ValueStateUpdate.getDefaultInstance();
    }

    private byte memoizedIsInitialized = -1;
    @java.lang.Override
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized == 1) return true;
      if (isInitialized == 0) return false;

      memoizedIsInitialized = 1;
      return true;
    }

    @java.lang.Override
    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      if (methodCase_ == 1) {
        output.writeMessage(1, (org.apache.spark.sql.execution.streaming.state.StateMessage.ValueStateExists) method_);
      }
      if (methodCase_ == 2) {
        output.writeMessage(2, (org.apache.spark.sql.execution.streaming.state.StateMessage.ValueStateGet) method_);
      }
      if (methodCase_ == 3) {
        output.writeMessage(3, (org.apache.spark.sql.execution.streaming.state.StateMessage.ValueStateUpdate) method_);
      }
      getUnknownFields().writeTo(output);
    }

    @java.lang.Override
    public int getSerializedSize() {
      int size = memoizedSize;
      if (size != -1) return size;

      size = 0;
      if (methodCase_ == 1) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(1, (org.apache.spark.sql.execution.streaming.state.StateMessage.ValueStateExists) method_);
      }
      if (methodCase_ == 2) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(2, (org.apache.spark.sql.execution.streaming.state.StateMessage.ValueStateGet) method_);
      }
      if (methodCase_ == 3) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(3, (org.apache.spark.sql.execution.streaming.state.StateMessage.ValueStateUpdate) method_);
      }
      size += getUnknownFields().getSerializedSize();
      memoizedSize = size;
      return size;
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.spark.sql.execution.streaming.state.StateMessage.ValueStateCall)) {
        return super.equals(obj);
      }
      org.apache.spark.sql.execution.streaming.state.StateMessage.ValueStateCall other = (org.apache.spark.sql.execution.streaming.state.StateMessage.ValueStateCall) obj;

      if (!getMethodCase().equals(other.getMethodCase())) return false;
      switch (methodCase_) {
        case 1:
          if (!getExists()
              .equals(other.getExists())) return false;
          break;
        case 2:
          if (!getGet()
              .equals(other.getGet())) return false;
          break;
        case 3:
          if (!getUpdate()
              .equals(other.getUpdate())) return false;
          break;
        case 0:
        default:
      }
      if (!getUnknownFields().equals(other.getUnknownFields())) return false;
      return true;
    }

    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptor().hashCode();
      switch (methodCase_) {
        case 1:
          hash = (37 * hash) + EXISTS_FIELD_NUMBER;
          hash = (53 * hash) + getExists().hashCode();
          break;
        case 2:
          hash = (37 * hash) + GET_FIELD_NUMBER;
          hash = (53 * hash) + getGet().hashCode();
          break;
        case 3:
          hash = (37 * hash) + UPDATE_FIELD_NUMBER;
          hash = (53 * hash) + getUpdate().hashCode();
          break;
        case 0:
        default:
      }
      hash = (29 * hash) + getUnknownFields().hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.spark.sql.execution.streaming.state.StateMessage.ValueStateCall parseFrom(
        java.nio.ByteBuffer data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.spark.sql.execution.streaming.state.StateMessage.ValueStateCall parseFrom(
        java.nio.ByteBuffer data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.spark.sql.execution.streaming.state.StateMessage.ValueStateCall parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.spark.sql.execution.streaming.state.StateMessage.ValueStateCall parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.spark.sql.execution.streaming.state.StateMessage.ValueStateCall parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.spark.sql.execution.streaming.state.StateMessage.ValueStateCall parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.spark.sql.execution.streaming.state.StateMessage.ValueStateCall parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.spark.sql.execution.streaming.state.StateMessage.ValueStateCall parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }

    public static org.apache.spark.sql.execution.streaming.state.StateMessage.ValueStateCall parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input);
    }

    public static org.apache.spark.sql.execution.streaming.state.StateMessage.ValueStateCall parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.spark.sql.execution.streaming.state.StateMessage.ValueStateCall parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.spark.sql.execution.streaming.state.StateMessage.ValueStateCall parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }

    @java.lang.Override
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder() {
      return DEFAULT_INSTANCE.toBuilder();
    }
    public static Builder newBuilder(org.apache.spark.sql.execution.streaming.state.StateMessage.ValueStateCall prototype) {
      return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
    }
    @java.lang.Override
    public Builder toBuilder() {
      return this == DEFAULT_INSTANCE
          ? new Builder() : new Builder().mergeFrom(this);
    }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code org.apache.spark.sql.execution.streaming.state.ValueStateCall}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessageV3.Builder<Builder> implements
        // @@protoc_insertion_point(builder_implements:org.apache.spark.sql.execution.streaming.state.ValueStateCall)
        org.apache.spark.sql.execution.streaming.state.StateMessage.ValueStateCallOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.spark.sql.execution.streaming.state.StateMessage.internal_static_org_apache_spark_sql_execution_streaming_state_ValueStateCall_descriptor;
      }

      @java.lang.Override
      protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.spark.sql.execution.streaming.state.StateMessage.internal_static_org_apache_spark_sql_execution_streaming_state_ValueStateCall_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.spark.sql.execution.streaming.state.StateMessage.ValueStateCall.class, org.apache.spark.sql.execution.streaming.state.StateMessage.ValueStateCall.Builder.class);
      }

      // Construct using org.apache.spark.sql.execution.streaming.state.StateMessage.ValueStateCall.newBuilder()
      private Builder() {

      }

      private Builder(
          com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
        super(parent);

      }
      @java.lang.Override
      public Builder clear() {
        super.clear();
        bitField0_ = 0;
        if (existsBuilder_ != null) {
          existsBuilder_.clear();
        }
        if (getBuilder_ != null) {
          getBuilder_.clear();
        }
        if (updateBuilder_ != null) {
          updateBuilder_.clear();
        }
        methodCase_ = 0;
        method_ = null;
        return this;
      }

      @java.lang.Override
      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.spark.sql.execution.streaming.state.StateMessage.internal_static_org_apache_spark_sql_execution_streaming_state_ValueStateCall_descriptor;
      }

      @java.lang.Override
      public org.apache.spark.sql.execution.streaming.state.StateMessage.ValueStateCall getDefaultInstanceForType() {
        return org.apache.spark.sql.execution.streaming.state.StateMessage.ValueStateCall.getDefaultInstance();
      }

      @java.lang.Override
      public org.apache.spark.sql.execution.streaming.state.StateMessage.ValueStateCall build() {
        org.apache.spark.sql.execution.streaming.state.StateMessage.ValueStateCall result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      @java.lang.Override
      public org.apache.spark.sql.execution.streaming.state.StateMessage.ValueStateCall buildPartial() {
        org.apache.spark.sql.execution.streaming.state.StateMessage.ValueStateCall result = new org.apache.spark.sql.execution.streaming.state.StateMessage.ValueStateCall(this);
        if (bitField0_ != 0) { buildPartial0(result); }
        buildPartialOneofs(result);
        onBuilt();
        return result;
      }

      private void buildPartial0(org.apache.spark.sql.execution.streaming.state.StateMessage.ValueStateCall result) {
        int from_bitField0_ = bitField0_;
      }

      private void buildPartialOneofs(org.apache.spark.sql.execution.streaming.state.StateMessage.ValueStateCall result) {
        result.methodCase_ = methodCase_;
        result.method_ = this.method_;
        if (methodCase_ == 1 &&
            existsBuilder_ != null) {
          result.method_ = existsBuilder_.build();
        }
        if (methodCase_ == 2 &&
            getBuilder_ != null) {
          result.method_ = getBuilder_.build();
        }
        if (methodCase_ == 3 &&
            updateBuilder_ != null) {
          result.method_ = updateBuilder_.build();
        }
      }

      @java.lang.Override
      public Builder clone() {
        return super.clone();
      }
      @java.lang.Override
      public Builder setField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.setField(field, value);
      }
      @java.lang.Override
      public Builder clearField(
          com.google.protobuf.Descriptors.FieldDescriptor field) {
        return super.clearField(field);
      }
      @java.lang.Override
      public Builder clearOneof(
          com.google.protobuf.Descriptors.OneofDescriptor oneof) {
        return super.clearOneof(oneof);
      }
      @java.lang.Override
      public Builder setRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          int index, java.lang.Object value) {
        return super.setRepeatedField(field, index, value);
      }
      @java.lang.Override
      public Builder addRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.addRepeatedField(field, value);
      }
      @java.lang.Override
      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.spark.sql.execution.streaming.state.StateMessage.ValueStateCall) {
          return mergeFrom((org.apache.spark.sql.execution.streaming.state.StateMessage.ValueStateCall)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.spark.sql.execution.streaming.state.StateMessage.ValueStateCall other) {
        if (other == org.apache.spark.sql.execution.streaming.state.StateMessage.ValueStateCall.getDefaultInstance()) return this;
        switch (other.getMethodCase()) {
          case EXISTS: {
            mergeExists(other.getExists());
            break;
          }
          case GET: {
            mergeGet(other.getGet());
            break;
          }
          case UPDATE: {
            mergeUpdate(other.getUpdate());
            break;
          }
          case METHOD_NOT_SET: {
            break;
          }
        }
        this.mergeUnknownFields(other.getUnknownFields());
        onChanged();
        return this;
      }

      @java.lang.Override
      public final boolean isInitialized() {
        return true;
      }

      @java.lang.Override
      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        if (extensionRegistry == null) {
          throw new java.lang.NullPointerException();
        }
        try {
          boolean done = false;
          while (!done) {
            int tag = input.readTag();
            switch (tag) {
              case 0:
                done = true;
                break;
              case 10: {
                input.readMessage(
                    getExistsFieldBuilder().getBuilder(),
                    extensionRegistry);
                methodCase_ = 1;
                break;
              } // case 10
              case 18: {
                input.readMessage(
                    getGetFieldBuilder().getBuilder(),
                    extensionRegistry);
                methodCase_ = 2;
                break;
              } // case 18
              case 26: {
                input.readMessage(
                    getUpdateFieldBuilder().getBuilder(),
                    extensionRegistry);
                methodCase_ = 3;
                break;
              } // case 26
              default: {
                if (!super.parseUnknownField(input, extensionRegistry, tag)) {
                  done = true; // was an endgroup tag
                }
                break;
              } // default:
            } // switch (tag)
          } // while (!done)
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          throw e.unwrapIOException();
        } finally {
          onChanged();
        } // finally
        return this;
      }
      private int methodCase_ = 0;
      private java.lang.Object method_;
      public MethodCase
          getMethodCase() {
        return MethodCase.forNumber(
            methodCase_);
      }

      public Builder clearMethod() {
        methodCase_ = 0;
        method_ = null;
        onChanged();
        return this;
      }

      private int bitField0_;

      private com.google.protobuf.SingleFieldBuilderV3<
          org.apache.spark.sql.execution.streaming.state.StateMessage.ValueStateExists, org.apache.spark.sql.execution.streaming.state.StateMessage.ValueStateExists.Builder, org.apache.spark.sql.execution.streaming.state.StateMessage.ValueStateExistsOrBuilder> existsBuilder_;
      /**
       * <code>.org.apache.spark.sql.execution.streaming.state.ValueStateExists exists = 1;</code>
       * @return Whether the exists field is set.
       */
      @java.lang.Override
      public boolean hasExists() {
        return methodCase_ == 1;
      }
      /**
       * <code>.org.apache.spark.sql.execution.streaming.state.ValueStateExists exists = 1;</code>
       * @return The exists.
       */
      @java.lang.Override
      public org.apache.spark.sql.execution.streaming.state.StateMessage.ValueStateExists getExists() {
        if (existsBuilder_ == null) {
          if (methodCase_ == 1) {
            return (org.apache.spark.sql.execution.streaming.state.StateMessage.ValueStateExists) method_;
          }
          return org.apache.spark.sql.execution.streaming.state.StateMessage.ValueStateExists.getDefaultInstance();
        } else {
          if (methodCase_ == 1) {
            return existsBuilder_.getMessage();
          }
          return org.apache.spark.sql.execution.streaming.state.StateMessage.ValueStateExists.getDefaultInstance();
        }
      }
      /**
       * <code>.org.apache.spark.sql.execution.streaming.state.ValueStateExists exists = 1;</code>
       */
      public Builder setExists(org.apache.spark.sql.execution.streaming.state.StateMessage.ValueStateExists value) {
        if (existsBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          method_ = value;
          onChanged();
        } else {
          existsBuilder_.setMessage(value);
        }
        methodCase_ = 1;
        return this;
      }
      /**
       * <code>.org.apache.spark.sql.execution.streaming.state.ValueStateExists exists = 1;</code>
       */
      public Builder setExists(
          org.apache.spark.sql.execution.streaming.state.StateMessage.ValueStateExists.Builder builderForValue) {
        if (existsBuilder_ == null) {
          method_ = builderForValue.build();
          onChanged();
        } else {
          existsBuilder_.setMessage(builderForValue.build());
        }
        methodCase_ = 1;
        return this;
      }
      /**
       * <code>.org.apache.spark.sql.execution.streaming.state.ValueStateExists exists = 1;</code>
       */
      public Builder mergeExists(org.apache.spark.sql.execution.streaming.state.StateMessage.ValueStateExists value) {
        if (existsBuilder_ == null) {
          if (methodCase_ == 1 &&
              method_ != org.apache.spark.sql.execution.streaming.state.StateMessage.ValueStateExists.getDefaultInstance()) {
            method_ = org.apache.spark.sql.execution.streaming.state.StateMessage.ValueStateExists.newBuilder((org.apache.spark.sql.execution.streaming.state.StateMessage.ValueStateExists) method_)
                .mergeFrom(value).buildPartial();
          } else {
            method_ = value;
          }
          onChanged();
        } else {
          if (methodCase_ == 1) {
            existsBuilder_.mergeFrom(value);
          } else {
            existsBuilder_.setMessage(value);
          }
        }
        methodCase_ = 1;
        return this;
      }
      /**
       * <code>.org.apache.spark.sql.execution.streaming.state.ValueStateExists exists = 1;</code>
       */
      public Builder clearExists() {
        if (existsBuilder_ == null) {
          if (methodCase_ == 1) {
            methodCase_ = 0;
            method_ = null;
            onChanged();
          }
        } else {
          if (methodCase_ == 1) {
            methodCase_ = 0;
            method_ = null;
          }
          existsBuilder_.clear();
        }
        return this;
      }
      /**
       * <code>.org.apache.spark.sql.execution.streaming.state.ValueStateExists exists = 1;</code>
       */
      public org.apache.spark.sql.execution.streaming.state.StateMessage.ValueStateExists.Builder getExistsBuilder() {
        return getExistsFieldBuilder().getBuilder();
      }
      /**
       * <code>.org.apache.spark.sql.execution.streaming.state.ValueStateExists exists = 1;</code>
       */
      @java.lang.Override
      public org.apache.spark.sql.execution.streaming.state.StateMessage.ValueStateExistsOrBuilder getExistsOrBuilder() {
        if ((methodCase_ == 1) && (existsBuilder_ != null)) {
          return existsBuilder_.getMessageOrBuilder();
        } else {
          if (methodCase_ == 1) {
            return (org.apache.spark.sql.execution.streaming.state.StateMessage.ValueStateExists) method_;
          }
          return org.apache.spark.sql.execution.streaming.state.StateMessage.ValueStateExists.getDefaultInstance();
        }
      }
      /**
       * <code>.org.apache.spark.sql.execution.streaming.state.ValueStateExists exists = 1;</code>
       */
      private com.google.protobuf.SingleFieldBuilderV3<
          org.apache.spark.sql.execution.streaming.state.StateMessage.ValueStateExists, org.apache.spark.sql.execution.streaming.state.StateMessage.ValueStateExists.Builder, org.apache.spark.sql.execution.streaming.state.StateMessage.ValueStateExistsOrBuilder> 
          getExistsFieldBuilder() {
        if (existsBuilder_ == null) {
          if (!(methodCase_ == 1)) {
            method_ = org.apache.spark.sql.execution.streaming.state.StateMessage.ValueStateExists.getDefaultInstance();
          }
          existsBuilder_ = new com.google.protobuf.SingleFieldBuilderV3<
              org.apache.spark.sql.execution.streaming.state.StateMessage.ValueStateExists, org.apache.spark.sql.execution.streaming.state.StateMessage.ValueStateExists.Builder, org.apache.spark.sql.execution.streaming.state.StateMessage.ValueStateExistsOrBuilder>(
                  (org.apache.spark.sql.execution.streaming.state.StateMessage.ValueStateExists) method_,
                  getParentForChildren(),
                  isClean());
          method_ = null;
        }
        methodCase_ = 1;
        onChanged();
        return existsBuilder_;
      }

      private com.google.protobuf.SingleFieldBuilderV3<
          org.apache.spark.sql.execution.streaming.state.StateMessage.ValueStateGet, org.apache.spark.sql.execution.streaming.state.StateMessage.ValueStateGet.Builder, org.apache.spark.sql.execution.streaming.state.StateMessage.ValueStateGetOrBuilder> getBuilder_;
      /**
       * <code>.org.apache.spark.sql.execution.streaming.state.ValueStateGet get = 2;</code>
       * @return Whether the get field is set.
       */
      @java.lang.Override
      public boolean hasGet() {
        return methodCase_ == 2;
      }
      /**
       * <code>.org.apache.spark.sql.execution.streaming.state.ValueStateGet get = 2;</code>
       * @return The get.
       */
      @java.lang.Override
      public org.apache.spark.sql.execution.streaming.state.StateMessage.ValueStateGet getGet() {
        if (getBuilder_ == null) {
          if (methodCase_ == 2) {
            return (org.apache.spark.sql.execution.streaming.state.StateMessage.ValueStateGet) method_;
          }
          return org.apache.spark.sql.execution.streaming.state.StateMessage.ValueStateGet.getDefaultInstance();
        } else {
          if (methodCase_ == 2) {
            return getBuilder_.getMessage();
          }
          return org.apache.spark.sql.execution.streaming.state.StateMessage.ValueStateGet.getDefaultInstance();
        }
      }
      /**
       * <code>.org.apache.spark.sql.execution.streaming.state.ValueStateGet get = 2;</code>
       */
      public Builder setGet(org.apache.spark.sql.execution.streaming.state.StateMessage.ValueStateGet value) {
        if (getBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          method_ = value;
          onChanged();
        } else {
          getBuilder_.setMessage(value);
        }
        methodCase_ = 2;
        return this;
      }
      /**
       * <code>.org.apache.spark.sql.execution.streaming.state.ValueStateGet get = 2;</code>
       */
      public Builder setGet(
          org.apache.spark.sql.execution.streaming.state.StateMessage.ValueStateGet.Builder builderForValue) {
        if (getBuilder_ == null) {
          method_ = builderForValue.build();
          onChanged();
        } else {
          getBuilder_.setMessage(builderForValue.build());
        }
        methodCase_ = 2;
        return this;
      }
      /**
       * <code>.org.apache.spark.sql.execution.streaming.state.ValueStateGet get = 2;</code>
       */
      public Builder mergeGet(org.apache.spark.sql.execution.streaming.state.StateMessage.ValueStateGet value) {
        if (getBuilder_ == null) {
          if (methodCase_ == 2 &&
              method_ != org.apache.spark.sql.execution.streaming.state.StateMessage.ValueStateGet.getDefaultInstance()) {
            method_ = org.apache.spark.sql.execution.streaming.state.StateMessage.ValueStateGet.newBuilder((org.apache.spark.sql.execution.streaming.state.StateMessage.ValueStateGet) method_)
                .mergeFrom(value).buildPartial();
          } else {
            method_ = value;
          }
          onChanged();
        } else {
          if (methodCase_ == 2) {
            getBuilder_.mergeFrom(value);
          } else {
            getBuilder_.setMessage(value);
          }
        }
        methodCase_ = 2;
        return this;
      }
      /**
       * <code>.org.apache.spark.sql.execution.streaming.state.ValueStateGet get = 2;</code>
       */
      public Builder clearGet() {
        if (getBuilder_ == null) {
          if (methodCase_ == 2) {
            methodCase_ = 0;
            method_ = null;
            onChanged();
          }
        } else {
          if (methodCase_ == 2) {
            methodCase_ = 0;
            method_ = null;
          }
          getBuilder_.clear();
        }
        return this;
      }
      /**
       * <code>.org.apache.spark.sql.execution.streaming.state.ValueStateGet get = 2;</code>
       */
      public org.apache.spark.sql.execution.streaming.state.StateMessage.ValueStateGet.Builder getGetBuilder() {
        return getGetFieldBuilder().getBuilder();
      }
      /**
       * <code>.org.apache.spark.sql.execution.streaming.state.ValueStateGet get = 2;</code>
       */
      @java.lang.Override
      public org.apache.spark.sql.execution.streaming.state.StateMessage.ValueStateGetOrBuilder getGetOrBuilder() {
        if ((methodCase_ == 2) && (getBuilder_ != null)) {
          return getBuilder_.getMessageOrBuilder();
        } else {
          if (methodCase_ == 2) {
            return (org.apache.spark.sql.execution.streaming.state.StateMessage.ValueStateGet) method_;
          }
          return org.apache.spark.sql.execution.streaming.state.StateMessage.ValueStateGet.getDefaultInstance();
        }
      }
      /**
       * <code>.org.apache.spark.sql.execution.streaming.state.ValueStateGet get = 2;</code>
       */
      private com.google.protobuf.SingleFieldBuilderV3<
          org.apache.spark.sql.execution.streaming.state.StateMessage.ValueStateGet, org.apache.spark.sql.execution.streaming.state.StateMessage.ValueStateGet.Builder, org.apache.spark.sql.execution.streaming.state.StateMessage.ValueStateGetOrBuilder> 
          getGetFieldBuilder() {
        if (getBuilder_ == null) {
          if (!(methodCase_ == 2)) {
            method_ = org.apache.spark.sql.execution.streaming.state.StateMessage.ValueStateGet.getDefaultInstance();
          }
          getBuilder_ = new com.google.protobuf.SingleFieldBuilderV3<
              org.apache.spark.sql.execution.streaming.state.StateMessage.ValueStateGet, org.apache.spark.sql.execution.streaming.state.StateMessage.ValueStateGet.Builder, org.apache.spark.sql.execution.streaming.state.StateMessage.ValueStateGetOrBuilder>(
                  (org.apache.spark.sql.execution.streaming.state.StateMessage.ValueStateGet) method_,
                  getParentForChildren(),
                  isClean());
          method_ = null;
        }
        methodCase_ = 2;
        onChanged();
        return getBuilder_;
      }

      private com.google.protobuf.SingleFieldBuilderV3<
          org.apache.spark.sql.execution.streaming.state.StateMessage.ValueStateUpdate, org.apache.spark.sql.execution.streaming.state.StateMessage.ValueStateUpdate.Builder, org.apache.spark.sql.execution.streaming.state.StateMessage.ValueStateUpdateOrBuilder> updateBuilder_;
      /**
       * <code>.org.apache.spark.sql.execution.streaming.state.ValueStateUpdate update = 3;</code>
       * @return Whether the update field is set.
       */
      @java.lang.Override
      public boolean hasUpdate() {
        return methodCase_ == 3;
      }
      /**
       * <code>.org.apache.spark.sql.execution.streaming.state.ValueStateUpdate update = 3;</code>
       * @return The update.
       */
      @java.lang.Override
      public org.apache.spark.sql.execution.streaming.state.StateMessage.ValueStateUpdate getUpdate() {
        if (updateBuilder_ == null) {
          if (methodCase_ == 3) {
            return (org.apache.spark.sql.execution.streaming.state.StateMessage.ValueStateUpdate) method_;
          }
          return org.apache.spark.sql.execution.streaming.state.StateMessage.ValueStateUpdate.getDefaultInstance();
        } else {
          if (methodCase_ == 3) {
            return updateBuilder_.getMessage();
          }
          return org.apache.spark.sql.execution.streaming.state.StateMessage.ValueStateUpdate.getDefaultInstance();
        }
      }
      /**
       * <code>.org.apache.spark.sql.execution.streaming.state.ValueStateUpdate update = 3;</code>
       */
      public Builder setUpdate(org.apache.spark.sql.execution.streaming.state.StateMessage.ValueStateUpdate value) {
        if (updateBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          method_ = value;
          onChanged();
        } else {
          updateBuilder_.setMessage(value);
        }
        methodCase_ = 3;
        return this;
      }
      /**
       * <code>.org.apache.spark.sql.execution.streaming.state.ValueStateUpdate update = 3;</code>
       */
      public Builder setUpdate(
          org.apache.spark.sql.execution.streaming.state.StateMessage.ValueStateUpdate.Builder builderForValue) {
        if (updateBuilder_ == null) {
          method_ = builderForValue.build();
          onChanged();
        } else {
          updateBuilder_.setMessage(builderForValue.build());
        }
        methodCase_ = 3;
        return this;
      }
      /**
       * <code>.org.apache.spark.sql.execution.streaming.state.ValueStateUpdate update = 3;</code>
       */
      public Builder mergeUpdate(org.apache.spark.sql.execution.streaming.state.StateMessage.ValueStateUpdate value) {
        if (updateBuilder_ == null) {
          if (methodCase_ == 3 &&
              method_ != org.apache.spark.sql.execution.streaming.state.StateMessage.ValueStateUpdate.getDefaultInstance()) {
            method_ = org.apache.spark.sql.execution.streaming.state.StateMessage.ValueStateUpdate.newBuilder((org.apache.spark.sql.execution.streaming.state.StateMessage.ValueStateUpdate) method_)
                .mergeFrom(value).buildPartial();
          } else {
            method_ = value;
          }
          onChanged();
        } else {
          if (methodCase_ == 3) {
            updateBuilder_.mergeFrom(value);
          } else {
            updateBuilder_.setMessage(value);
          }
        }
        methodCase_ = 3;
        return this;
      }
      /**
       * <code>.org.apache.spark.sql.execution.streaming.state.ValueStateUpdate update = 3;</code>
       */
      public Builder clearUpdate() {
        if (updateBuilder_ == null) {
          if (methodCase_ == 3) {
            methodCase_ = 0;
            method_ = null;
            onChanged();
          }
        } else {
          if (methodCase_ == 3) {
            methodCase_ = 0;
            method_ = null;
          }
          updateBuilder_.clear();
        }
        return this;
      }
      /**
       * <code>.org.apache.spark.sql.execution.streaming.state.ValueStateUpdate update = 3;</code>
       */
      public org.apache.spark.sql.execution.streaming.state.StateMessage.ValueStateUpdate.Builder getUpdateBuilder() {
        return getUpdateFieldBuilder().getBuilder();
      }
      /**
       * <code>.org.apache.spark.sql.execution.streaming.state.ValueStateUpdate update = 3;</code>
       */
      @java.lang.Override
      public org.apache.spark.sql.execution.streaming.state.StateMessage.ValueStateUpdateOrBuilder getUpdateOrBuilder() {
        if ((methodCase_ == 3) && (updateBuilder_ != null)) {
          return updateBuilder_.getMessageOrBuilder();
        } else {
          if (methodCase_ == 3) {
            return (org.apache.spark.sql.execution.streaming.state.StateMessage.ValueStateUpdate) method_;
          }
          return org.apache.spark.sql.execution.streaming.state.StateMessage.ValueStateUpdate.getDefaultInstance();
        }
      }
      /**
       * <code>.org.apache.spark.sql.execution.streaming.state.ValueStateUpdate update = 3;</code>
       */
      private com.google.protobuf.SingleFieldBuilderV3<
          org.apache.spark.sql.execution.streaming.state.StateMessage.ValueStateUpdate, org.apache.spark.sql.execution.streaming.state.StateMessage.ValueStateUpdate.Builder, org.apache.spark.sql.execution.streaming.state.StateMessage.ValueStateUpdateOrBuilder> 
          getUpdateFieldBuilder() {
        if (updateBuilder_ == null) {
          if (!(methodCase_ == 3)) {
            method_ = org.apache.spark.sql.execution.streaming.state.StateMessage.ValueStateUpdate.getDefaultInstance();
          }
          updateBuilder_ = new com.google.protobuf.SingleFieldBuilderV3<
              org.apache.spark.sql.execution.streaming.state.StateMessage.ValueStateUpdate, org.apache.spark.sql.execution.streaming.state.StateMessage.ValueStateUpdate.Builder, org.apache.spark.sql.execution.streaming.state.StateMessage.ValueStateUpdateOrBuilder>(
                  (org.apache.spark.sql.execution.streaming.state.StateMessage.ValueStateUpdate) method_,
                  getParentForChildren(),
                  isClean());
          method_ = null;
        }
        methodCase_ = 3;
        onChanged();
        return updateBuilder_;
      }
      @java.lang.Override
      public final Builder setUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.setUnknownFields(unknownFields);
      }

      @java.lang.Override
      public final Builder mergeUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.mergeUnknownFields(unknownFields);
      }


      // @@protoc_insertion_point(builder_scope:org.apache.spark.sql.execution.streaming.state.ValueStateCall)
    }

    // @@protoc_insertion_point(class_scope:org.apache.spark.sql.execution.streaming.state.ValueStateCall)
    private static final org.apache.spark.sql.execution.streaming.state.StateMessage.ValueStateCall DEFAULT_INSTANCE;
    static {
      DEFAULT_INSTANCE = new org.apache.spark.sql.execution.streaming.state.StateMessage.ValueStateCall();
    }

    public static org.apache.spark.sql.execution.streaming.state.StateMessage.ValueStateCall getDefaultInstance() {
      return DEFAULT_INSTANCE;
    }

    private static final com.google.protobuf.Parser<ValueStateCall>
        PARSER = new com.google.protobuf.AbstractParser<ValueStateCall>() {
      @java.lang.Override
      public ValueStateCall parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        Builder builder = newBuilder();
        try {
          builder.mergeFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          throw e.setUnfinishedMessage(builder.buildPartial());
        } catch (com.google.protobuf.UninitializedMessageException e) {
          throw e.asInvalidProtocolBufferException().setUnfinishedMessage(builder.buildPartial());
        } catch (java.io.IOException e) {
          throw new com.google.protobuf.InvalidProtocolBufferException(e)
              .setUnfinishedMessage(builder.buildPartial());
        }
        return builder.buildPartial();
      }
    };

    public static com.google.protobuf.Parser<ValueStateCall> parser() {
      return PARSER;
    }

    @java.lang.Override
    public com.google.protobuf.Parser<ValueStateCall> getParserForType() {
      return PARSER;
    }

    @java.lang.Override
    public org.apache.spark.sql.execution.streaming.state.StateMessage.ValueStateCall getDefaultInstanceForType() {
      return DEFAULT_INSTANCE;
    }

  }

  public interface ValueStateExistsOrBuilder extends
      // @@protoc_insertion_point(interface_extends:org.apache.spark.sql.execution.streaming.state.ValueStateExists)
      com.google.protobuf.MessageOrBuilder {
  }
  /**
   * Protobuf type {@code org.apache.spark.sql.execution.streaming.state.ValueStateExists}
   */
  public static final class ValueStateExists extends
      com.google.protobuf.GeneratedMessageV3 implements
      // @@protoc_insertion_point(message_implements:org.apache.spark.sql.execution.streaming.state.ValueStateExists)
      ValueStateExistsOrBuilder {
  private static final long serialVersionUID = 0L;
    // Use ValueStateExists.newBuilder() to construct.
    private ValueStateExists(com.google.protobuf.GeneratedMessageV3.Builder<?> builder) {
      super(builder);
    }
    private ValueStateExists() {
    }

    @java.lang.Override
    @SuppressWarnings({"unused"})
    protected java.lang.Object newInstance(
        UnusedPrivateParameter unused) {
      return new ValueStateExists();
    }

    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.spark.sql.execution.streaming.state.StateMessage.internal_static_org_apache_spark_sql_execution_streaming_state_ValueStateExists_descriptor;
    }

    @java.lang.Override
    protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.spark.sql.execution.streaming.state.StateMessage.internal_static_org_apache_spark_sql_execution_streaming_state_ValueStateExists_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.spark.sql.execution.streaming.state.StateMessage.ValueStateExists.class, org.apache.spark.sql.execution.streaming.state.StateMessage.ValueStateExists.Builder.class);
    }

    private byte memoizedIsInitialized = -1;
    @java.lang.Override
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized == 1) return true;
      if (isInitialized == 0) return false;

      memoizedIsInitialized = 1;
      return true;
    }

    @java.lang.Override
    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      getUnknownFields().writeTo(output);
    }

    @java.lang.Override
    public int getSerializedSize() {
      int size = memoizedSize;
      if (size != -1) return size;

      size = 0;
      size += getUnknownFields().getSerializedSize();
      memoizedSize = size;
      return size;
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.spark.sql.execution.streaming.state.StateMessage.ValueStateExists)) {
        return super.equals(obj);
      }
      org.apache.spark.sql.execution.streaming.state.StateMessage.ValueStateExists other = (org.apache.spark.sql.execution.streaming.state.StateMessage.ValueStateExists) obj;

      if (!getUnknownFields().equals(other.getUnknownFields())) return false;
      return true;
    }

    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptor().hashCode();
      hash = (29 * hash) + getUnknownFields().hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.spark.sql.execution.streaming.state.StateMessage.ValueStateExists parseFrom(
        java.nio.ByteBuffer data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.spark.sql.execution.streaming.state.StateMessage.ValueStateExists parseFrom(
        java.nio.ByteBuffer data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.spark.sql.execution.streaming.state.StateMessage.ValueStateExists parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.spark.sql.execution.streaming.state.StateMessage.ValueStateExists parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.spark.sql.execution.streaming.state.StateMessage.ValueStateExists parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.spark.sql.execution.streaming.state.StateMessage.ValueStateExists parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.spark.sql.execution.streaming.state.StateMessage.ValueStateExists parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.spark.sql.execution.streaming.state.StateMessage.ValueStateExists parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }

    public static org.apache.spark.sql.execution.streaming.state.StateMessage.ValueStateExists parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input);
    }

    public static org.apache.spark.sql.execution.streaming.state.StateMessage.ValueStateExists parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.spark.sql.execution.streaming.state.StateMessage.ValueStateExists parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.spark.sql.execution.streaming.state.StateMessage.ValueStateExists parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }

    @java.lang.Override
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder() {
      return DEFAULT_INSTANCE.toBuilder();
    }
    public static Builder newBuilder(org.apache.spark.sql.execution.streaming.state.StateMessage.ValueStateExists prototype) {
      return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
    }
    @java.lang.Override
    public Builder toBuilder() {
      return this == DEFAULT_INSTANCE
          ? new Builder() : new Builder().mergeFrom(this);
    }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code org.apache.spark.sql.execution.streaming.state.ValueStateExists}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessageV3.Builder<Builder> implements
        // @@protoc_insertion_point(builder_implements:org.apache.spark.sql.execution.streaming.state.ValueStateExists)
        org.apache.spark.sql.execution.streaming.state.StateMessage.ValueStateExistsOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.spark.sql.execution.streaming.state.StateMessage.internal_static_org_apache_spark_sql_execution_streaming_state_ValueStateExists_descriptor;
      }

      @java.lang.Override
      protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.spark.sql.execution.streaming.state.StateMessage.internal_static_org_apache_spark_sql_execution_streaming_state_ValueStateExists_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.spark.sql.execution.streaming.state.StateMessage.ValueStateExists.class, org.apache.spark.sql.execution.streaming.state.StateMessage.ValueStateExists.Builder.class);
      }

      // Construct using org.apache.spark.sql.execution.streaming.state.StateMessage.ValueStateExists.newBuilder()
      private Builder() {

      }

      private Builder(
          com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
        super(parent);

      }
      @java.lang.Override
      public Builder clear() {
        super.clear();
        return this;
      }

      @java.lang.Override
      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.spark.sql.execution.streaming.state.StateMessage.internal_static_org_apache_spark_sql_execution_streaming_state_ValueStateExists_descriptor;
      }

      @java.lang.Override
      public org.apache.spark.sql.execution.streaming.state.StateMessage.ValueStateExists getDefaultInstanceForType() {
        return org.apache.spark.sql.execution.streaming.state.StateMessage.ValueStateExists.getDefaultInstance();
      }

      @java.lang.Override
      public org.apache.spark.sql.execution.streaming.state.StateMessage.ValueStateExists build() {
        org.apache.spark.sql.execution.streaming.state.StateMessage.ValueStateExists result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      @java.lang.Override
      public org.apache.spark.sql.execution.streaming.state.StateMessage.ValueStateExists buildPartial() {
        org.apache.spark.sql.execution.streaming.state.StateMessage.ValueStateExists result = new org.apache.spark.sql.execution.streaming.state.StateMessage.ValueStateExists(this);
        onBuilt();
        return result;
      }

      @java.lang.Override
      public Builder clone() {
        return super.clone();
      }
      @java.lang.Override
      public Builder setField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.setField(field, value);
      }
      @java.lang.Override
      public Builder clearField(
          com.google.protobuf.Descriptors.FieldDescriptor field) {
        return super.clearField(field);
      }
      @java.lang.Override
      public Builder clearOneof(
          com.google.protobuf.Descriptors.OneofDescriptor oneof) {
        return super.clearOneof(oneof);
      }
      @java.lang.Override
      public Builder setRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          int index, java.lang.Object value) {
        return super.setRepeatedField(field, index, value);
      }
      @java.lang.Override
      public Builder addRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.addRepeatedField(field, value);
      }
      @java.lang.Override
      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.spark.sql.execution.streaming.state.StateMessage.ValueStateExists) {
          return mergeFrom((org.apache.spark.sql.execution.streaming.state.StateMessage.ValueStateExists)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.spark.sql.execution.streaming.state.StateMessage.ValueStateExists other) {
        if (other == org.apache.spark.sql.execution.streaming.state.StateMessage.ValueStateExists.getDefaultInstance()) return this;
        this.mergeUnknownFields(other.getUnknownFields());
        onChanged();
        return this;
      }

      @java.lang.Override
      public final boolean isInitialized() {
        return true;
      }

      @java.lang.Override
      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        if (extensionRegistry == null) {
          throw new java.lang.NullPointerException();
        }
        try {
          boolean done = false;
          while (!done) {
            int tag = input.readTag();
            switch (tag) {
              case 0:
                done = true;
                break;
              default: {
                if (!super.parseUnknownField(input, extensionRegistry, tag)) {
                  done = true; // was an endgroup tag
                }
                break;
              } // default:
            } // switch (tag)
          } // while (!done)
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          throw e.unwrapIOException();
        } finally {
          onChanged();
        } // finally
        return this;
      }
      @java.lang.Override
      public final Builder setUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.setUnknownFields(unknownFields);
      }

      @java.lang.Override
      public final Builder mergeUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.mergeUnknownFields(unknownFields);
      }


      // @@protoc_insertion_point(builder_scope:org.apache.spark.sql.execution.streaming.state.ValueStateExists)
    }

    // @@protoc_insertion_point(class_scope:org.apache.spark.sql.execution.streaming.state.ValueStateExists)
    private static final org.apache.spark.sql.execution.streaming.state.StateMessage.ValueStateExists DEFAULT_INSTANCE;
    static {
      DEFAULT_INSTANCE = new org.apache.spark.sql.execution.streaming.state.StateMessage.ValueStateExists();
    }

    public static org.apache.spark.sql.execution.streaming.state.StateMessage.ValueStateExists getDefaultInstance() {
      return DEFAULT_INSTANCE;
    }

    private static final com.google.protobuf.Parser<ValueStateExists>
        PARSER = new com.google.protobuf.AbstractParser<ValueStateExists>() {
      @java.lang.Override
      public ValueStateExists parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        Builder builder = newBuilder();
        try {
          builder.mergeFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          throw e.setUnfinishedMessage(builder.buildPartial());
        } catch (com.google.protobuf.UninitializedMessageException e) {
          throw e.asInvalidProtocolBufferException().setUnfinishedMessage(builder.buildPartial());
        } catch (java.io.IOException e) {
          throw new com.google.protobuf.InvalidProtocolBufferException(e)
              .setUnfinishedMessage(builder.buildPartial());
        }
        return builder.buildPartial();
      }
    };

    public static com.google.protobuf.Parser<ValueStateExists> parser() {
      return PARSER;
    }

    @java.lang.Override
    public com.google.protobuf.Parser<ValueStateExists> getParserForType() {
      return PARSER;
    }

    @java.lang.Override
    public org.apache.spark.sql.execution.streaming.state.StateMessage.ValueStateExists getDefaultInstanceForType() {
      return DEFAULT_INSTANCE;
    }

  }

  public interface ValueStateGetOrBuilder extends
      // @@protoc_insertion_point(interface_extends:org.apache.spark.sql.execution.streaming.state.ValueStateGet)
      com.google.protobuf.MessageOrBuilder {
  }
  /**
   * Protobuf type {@code org.apache.spark.sql.execution.streaming.state.ValueStateGet}
   */
  public static final class ValueStateGet extends
      com.google.protobuf.GeneratedMessageV3 implements
      // @@protoc_insertion_point(message_implements:org.apache.spark.sql.execution.streaming.state.ValueStateGet)
      ValueStateGetOrBuilder {
  private static final long serialVersionUID = 0L;
    // Use ValueStateGet.newBuilder() to construct.
    private ValueStateGet(com.google.protobuf.GeneratedMessageV3.Builder<?> builder) {
      super(builder);
    }
    private ValueStateGet() {
    }

    @java.lang.Override
    @SuppressWarnings({"unused"})
    protected java.lang.Object newInstance(
        UnusedPrivateParameter unused) {
      return new ValueStateGet();
    }

    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.spark.sql.execution.streaming.state.StateMessage.internal_static_org_apache_spark_sql_execution_streaming_state_ValueStateGet_descriptor;
    }

    @java.lang.Override
    protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.spark.sql.execution.streaming.state.StateMessage.internal_static_org_apache_spark_sql_execution_streaming_state_ValueStateGet_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.spark.sql.execution.streaming.state.StateMessage.ValueStateGet.class, org.apache.spark.sql.execution.streaming.state.StateMessage.ValueStateGet.Builder.class);
    }

    private byte memoizedIsInitialized = -1;
    @java.lang.Override
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized == 1) return true;
      if (isInitialized == 0) return false;

      memoizedIsInitialized = 1;
      return true;
    }

    @java.lang.Override
    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      getUnknownFields().writeTo(output);
    }

    @java.lang.Override
    public int getSerializedSize() {
      int size = memoizedSize;
      if (size != -1) return size;

      size = 0;
      size += getUnknownFields().getSerializedSize();
      memoizedSize = size;
      return size;
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.spark.sql.execution.streaming.state.StateMessage.ValueStateGet)) {
        return super.equals(obj);
      }
      org.apache.spark.sql.execution.streaming.state.StateMessage.ValueStateGet other = (org.apache.spark.sql.execution.streaming.state.StateMessage.ValueStateGet) obj;

      if (!getUnknownFields().equals(other.getUnknownFields())) return false;
      return true;
    }

    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptor().hashCode();
      hash = (29 * hash) + getUnknownFields().hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.spark.sql.execution.streaming.state.StateMessage.ValueStateGet parseFrom(
        java.nio.ByteBuffer data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.spark.sql.execution.streaming.state.StateMessage.ValueStateGet parseFrom(
        java.nio.ByteBuffer data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.spark.sql.execution.streaming.state.StateMessage.ValueStateGet parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.spark.sql.execution.streaming.state.StateMessage.ValueStateGet parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.spark.sql.execution.streaming.state.StateMessage.ValueStateGet parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.spark.sql.execution.streaming.state.StateMessage.ValueStateGet parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.spark.sql.execution.streaming.state.StateMessage.ValueStateGet parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.spark.sql.execution.streaming.state.StateMessage.ValueStateGet parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }

    public static org.apache.spark.sql.execution.streaming.state.StateMessage.ValueStateGet parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input);
    }

    public static org.apache.spark.sql.execution.streaming.state.StateMessage.ValueStateGet parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.spark.sql.execution.streaming.state.StateMessage.ValueStateGet parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.spark.sql.execution.streaming.state.StateMessage.ValueStateGet parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }

    @java.lang.Override
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder() {
      return DEFAULT_INSTANCE.toBuilder();
    }
    public static Builder newBuilder(org.apache.spark.sql.execution.streaming.state.StateMessage.ValueStateGet prototype) {
      return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
    }
    @java.lang.Override
    public Builder toBuilder() {
      return this == DEFAULT_INSTANCE
          ? new Builder() : new Builder().mergeFrom(this);
    }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code org.apache.spark.sql.execution.streaming.state.ValueStateGet}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessageV3.Builder<Builder> implements
        // @@protoc_insertion_point(builder_implements:org.apache.spark.sql.execution.streaming.state.ValueStateGet)
        org.apache.spark.sql.execution.streaming.state.StateMessage.ValueStateGetOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.spark.sql.execution.streaming.state.StateMessage.internal_static_org_apache_spark_sql_execution_streaming_state_ValueStateGet_descriptor;
      }

      @java.lang.Override
      protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.spark.sql.execution.streaming.state.StateMessage.internal_static_org_apache_spark_sql_execution_streaming_state_ValueStateGet_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.spark.sql.execution.streaming.state.StateMessage.ValueStateGet.class, org.apache.spark.sql.execution.streaming.state.StateMessage.ValueStateGet.Builder.class);
      }

      // Construct using org.apache.spark.sql.execution.streaming.state.StateMessage.ValueStateGet.newBuilder()
      private Builder() {

      }

      private Builder(
          com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
        super(parent);

      }
      @java.lang.Override
      public Builder clear() {
        super.clear();
        return this;
      }

      @java.lang.Override
      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.spark.sql.execution.streaming.state.StateMessage.internal_static_org_apache_spark_sql_execution_streaming_state_ValueStateGet_descriptor;
      }

      @java.lang.Override
      public org.apache.spark.sql.execution.streaming.state.StateMessage.ValueStateGet getDefaultInstanceForType() {
        return org.apache.spark.sql.execution.streaming.state.StateMessage.ValueStateGet.getDefaultInstance();
      }

      @java.lang.Override
      public org.apache.spark.sql.execution.streaming.state.StateMessage.ValueStateGet build() {
        org.apache.spark.sql.execution.streaming.state.StateMessage.ValueStateGet result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      @java.lang.Override
      public org.apache.spark.sql.execution.streaming.state.StateMessage.ValueStateGet buildPartial() {
        org.apache.spark.sql.execution.streaming.state.StateMessage.ValueStateGet result = new org.apache.spark.sql.execution.streaming.state.StateMessage.ValueStateGet(this);
        onBuilt();
        return result;
      }

      @java.lang.Override
      public Builder clone() {
        return super.clone();
      }
      @java.lang.Override
      public Builder setField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.setField(field, value);
      }
      @java.lang.Override
      public Builder clearField(
          com.google.protobuf.Descriptors.FieldDescriptor field) {
        return super.clearField(field);
      }
      @java.lang.Override
      public Builder clearOneof(
          com.google.protobuf.Descriptors.OneofDescriptor oneof) {
        return super.clearOneof(oneof);
      }
      @java.lang.Override
      public Builder setRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          int index, java.lang.Object value) {
        return super.setRepeatedField(field, index, value);
      }
      @java.lang.Override
      public Builder addRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.addRepeatedField(field, value);
      }
      @java.lang.Override
      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.spark.sql.execution.streaming.state.StateMessage.ValueStateGet) {
          return mergeFrom((org.apache.spark.sql.execution.streaming.state.StateMessage.ValueStateGet)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.spark.sql.execution.streaming.state.StateMessage.ValueStateGet other) {
        if (other == org.apache.spark.sql.execution.streaming.state.StateMessage.ValueStateGet.getDefaultInstance()) return this;
        this.mergeUnknownFields(other.getUnknownFields());
        onChanged();
        return this;
      }

      @java.lang.Override
      public final boolean isInitialized() {
        return true;
      }

      @java.lang.Override
      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        if (extensionRegistry == null) {
          throw new java.lang.NullPointerException();
        }
        try {
          boolean done = false;
          while (!done) {
            int tag = input.readTag();
            switch (tag) {
              case 0:
                done = true;
                break;
              default: {
                if (!super.parseUnknownField(input, extensionRegistry, tag)) {
                  done = true; // was an endgroup tag
                }
                break;
              } // default:
            } // switch (tag)
          } // while (!done)
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          throw e.unwrapIOException();
        } finally {
          onChanged();
        } // finally
        return this;
      }
      @java.lang.Override
      public final Builder setUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.setUnknownFields(unknownFields);
      }

      @java.lang.Override
      public final Builder mergeUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.mergeUnknownFields(unknownFields);
      }


      // @@protoc_insertion_point(builder_scope:org.apache.spark.sql.execution.streaming.state.ValueStateGet)
    }

    // @@protoc_insertion_point(class_scope:org.apache.spark.sql.execution.streaming.state.ValueStateGet)
    private static final org.apache.spark.sql.execution.streaming.state.StateMessage.ValueStateGet DEFAULT_INSTANCE;
    static {
      DEFAULT_INSTANCE = new org.apache.spark.sql.execution.streaming.state.StateMessage.ValueStateGet();
    }

    public static org.apache.spark.sql.execution.streaming.state.StateMessage.ValueStateGet getDefaultInstance() {
      return DEFAULT_INSTANCE;
    }

    private static final com.google.protobuf.Parser<ValueStateGet>
        PARSER = new com.google.protobuf.AbstractParser<ValueStateGet>() {
      @java.lang.Override
      public ValueStateGet parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        Builder builder = newBuilder();
        try {
          builder.mergeFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          throw e.setUnfinishedMessage(builder.buildPartial());
        } catch (com.google.protobuf.UninitializedMessageException e) {
          throw e.asInvalidProtocolBufferException().setUnfinishedMessage(builder.buildPartial());
        } catch (java.io.IOException e) {
          throw new com.google.protobuf.InvalidProtocolBufferException(e)
              .setUnfinishedMessage(builder.buildPartial());
        }
        return builder.buildPartial();
      }
    };

    public static com.google.protobuf.Parser<ValueStateGet> parser() {
      return PARSER;
    }

    @java.lang.Override
    public com.google.protobuf.Parser<ValueStateGet> getParserForType() {
      return PARSER;
    }

    @java.lang.Override
    public org.apache.spark.sql.execution.streaming.state.StateMessage.ValueStateGet getDefaultInstanceForType() {
      return DEFAULT_INSTANCE;
    }

  }

  public interface ValueStateUpdateOrBuilder extends
      // @@protoc_insertion_point(interface_extends:org.apache.spark.sql.execution.streaming.state.ValueStateUpdate)
      com.google.protobuf.MessageOrBuilder {

    /**
     * <code>bytes updatedValue = 1;</code>
     * @return The updatedValue.
     */
    com.google.protobuf.ByteString getUpdatedValue();
  }
  /**
   * Protobuf type {@code org.apache.spark.sql.execution.streaming.state.ValueStateUpdate}
   */
  public static final class ValueStateUpdate extends
      com.google.protobuf.GeneratedMessageV3 implements
      // @@protoc_insertion_point(message_implements:org.apache.spark.sql.execution.streaming.state.ValueStateUpdate)
      ValueStateUpdateOrBuilder {
  private static final long serialVersionUID = 0L;
    // Use ValueStateUpdate.newBuilder() to construct.
    private ValueStateUpdate(com.google.protobuf.GeneratedMessageV3.Builder<?> builder) {
      super(builder);
    }
    private ValueStateUpdate() {
      updatedValue_ = com.google.protobuf.ByteString.EMPTY;
    }

    @java.lang.Override
    @SuppressWarnings({"unused"})
    protected java.lang.Object newInstance(
        UnusedPrivateParameter unused) {
      return new ValueStateUpdate();
    }

    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.spark.sql.execution.streaming.state.StateMessage.internal_static_org_apache_spark_sql_execution_streaming_state_ValueStateUpdate_descriptor;
    }

    @java.lang.Override
    protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.spark.sql.execution.streaming.state.StateMessage.internal_static_org_apache_spark_sql_execution_streaming_state_ValueStateUpdate_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.spark.sql.execution.streaming.state.StateMessage.ValueStateUpdate.class, org.apache.spark.sql.execution.streaming.state.StateMessage.ValueStateUpdate.Builder.class);
    }

    public static final int UPDATEDVALUE_FIELD_NUMBER = 1;
    private com.google.protobuf.ByteString updatedValue_ = com.google.protobuf.ByteString.EMPTY;
    /**
     * <code>bytes updatedValue = 1;</code>
     * @return The updatedValue.
     */
    @java.lang.Override
    public com.google.protobuf.ByteString getUpdatedValue() {
      return updatedValue_;
    }

    private byte memoizedIsInitialized = -1;
    @java.lang.Override
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized == 1) return true;
      if (isInitialized == 0) return false;

      memoizedIsInitialized = 1;
      return true;
    }

    @java.lang.Override
    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      if (!updatedValue_.isEmpty()) {
        output.writeBytes(1, updatedValue_);
      }
      getUnknownFields().writeTo(output);
    }

    @java.lang.Override
    public int getSerializedSize() {
      int size = memoizedSize;
      if (size != -1) return size;

      size = 0;
      if (!updatedValue_.isEmpty()) {
        size += com.google.protobuf.CodedOutputStream
          .computeBytesSize(1, updatedValue_);
      }
      size += getUnknownFields().getSerializedSize();
      memoizedSize = size;
      return size;
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.spark.sql.execution.streaming.state.StateMessage.ValueStateUpdate)) {
        return super.equals(obj);
      }
      org.apache.spark.sql.execution.streaming.state.StateMessage.ValueStateUpdate other = (org.apache.spark.sql.execution.streaming.state.StateMessage.ValueStateUpdate) obj;

      if (!getUpdatedValue()
          .equals(other.getUpdatedValue())) return false;
      if (!getUnknownFields().equals(other.getUnknownFields())) return false;
      return true;
    }

    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptor().hashCode();
      hash = (37 * hash) + UPDATEDVALUE_FIELD_NUMBER;
      hash = (53 * hash) + getUpdatedValue().hashCode();
      hash = (29 * hash) + getUnknownFields().hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.spark.sql.execution.streaming.state.StateMessage.ValueStateUpdate parseFrom(
        java.nio.ByteBuffer data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.spark.sql.execution.streaming.state.StateMessage.ValueStateUpdate parseFrom(
        java.nio.ByteBuffer data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.spark.sql.execution.streaming.state.StateMessage.ValueStateUpdate parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.spark.sql.execution.streaming.state.StateMessage.ValueStateUpdate parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.spark.sql.execution.streaming.state.StateMessage.ValueStateUpdate parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.spark.sql.execution.streaming.state.StateMessage.ValueStateUpdate parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.spark.sql.execution.streaming.state.StateMessage.ValueStateUpdate parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.spark.sql.execution.streaming.state.StateMessage.ValueStateUpdate parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }

    public static org.apache.spark.sql.execution.streaming.state.StateMessage.ValueStateUpdate parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input);
    }

    public static org.apache.spark.sql.execution.streaming.state.StateMessage.ValueStateUpdate parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.spark.sql.execution.streaming.state.StateMessage.ValueStateUpdate parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.spark.sql.execution.streaming.state.StateMessage.ValueStateUpdate parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }

    @java.lang.Override
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder() {
      return DEFAULT_INSTANCE.toBuilder();
    }
    public static Builder newBuilder(org.apache.spark.sql.execution.streaming.state.StateMessage.ValueStateUpdate prototype) {
      return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
    }
    @java.lang.Override
    public Builder toBuilder() {
      return this == DEFAULT_INSTANCE
          ? new Builder() : new Builder().mergeFrom(this);
    }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code org.apache.spark.sql.execution.streaming.state.ValueStateUpdate}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessageV3.Builder<Builder> implements
        // @@protoc_insertion_point(builder_implements:org.apache.spark.sql.execution.streaming.state.ValueStateUpdate)
        org.apache.spark.sql.execution.streaming.state.StateMessage.ValueStateUpdateOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.spark.sql.execution.streaming.state.StateMessage.internal_static_org_apache_spark_sql_execution_streaming_state_ValueStateUpdate_descriptor;
      }

      @java.lang.Override
      protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.spark.sql.execution.streaming.state.StateMessage.internal_static_org_apache_spark_sql_execution_streaming_state_ValueStateUpdate_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.spark.sql.execution.streaming.state.StateMessage.ValueStateUpdate.class, org.apache.spark.sql.execution.streaming.state.StateMessage.ValueStateUpdate.Builder.class);
      }

      // Construct using org.apache.spark.sql.execution.streaming.state.StateMessage.ValueStateUpdate.newBuilder()
      private Builder() {

      }

      private Builder(
          com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
        super(parent);

      }
      @java.lang.Override
      public Builder clear() {
        super.clear();
        bitField0_ = 0;
        updatedValue_ = com.google.protobuf.ByteString.EMPTY;
        return this;
      }

      @java.lang.Override
      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.spark.sql.execution.streaming.state.StateMessage.internal_static_org_apache_spark_sql_execution_streaming_state_ValueStateUpdate_descriptor;
      }

      @java.lang.Override
      public org.apache.spark.sql.execution.streaming.state.StateMessage.ValueStateUpdate getDefaultInstanceForType() {
        return org.apache.spark.sql.execution.streaming.state.StateMessage.ValueStateUpdate.getDefaultInstance();
      }

      @java.lang.Override
      public org.apache.spark.sql.execution.streaming.state.StateMessage.ValueStateUpdate build() {
        org.apache.spark.sql.execution.streaming.state.StateMessage.ValueStateUpdate result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      @java.lang.Override
      public org.apache.spark.sql.execution.streaming.state.StateMessage.ValueStateUpdate buildPartial() {
        org.apache.spark.sql.execution.streaming.state.StateMessage.ValueStateUpdate result = new org.apache.spark.sql.execution.streaming.state.StateMessage.ValueStateUpdate(this);
        if (bitField0_ != 0) { buildPartial0(result); }
        onBuilt();
        return result;
      }

      private void buildPartial0(org.apache.spark.sql.execution.streaming.state.StateMessage.ValueStateUpdate result) {
        int from_bitField0_ = bitField0_;
        if (((from_bitField0_ & 0x00000001) != 0)) {
          result.updatedValue_ = updatedValue_;
        }
      }

      @java.lang.Override
      public Builder clone() {
        return super.clone();
      }
      @java.lang.Override
      public Builder setField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.setField(field, value);
      }
      @java.lang.Override
      public Builder clearField(
          com.google.protobuf.Descriptors.FieldDescriptor field) {
        return super.clearField(field);
      }
      @java.lang.Override
      public Builder clearOneof(
          com.google.protobuf.Descriptors.OneofDescriptor oneof) {
        return super.clearOneof(oneof);
      }
      @java.lang.Override
      public Builder setRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          int index, java.lang.Object value) {
        return super.setRepeatedField(field, index, value);
      }
      @java.lang.Override
      public Builder addRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.addRepeatedField(field, value);
      }
      @java.lang.Override
      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.spark.sql.execution.streaming.state.StateMessage.ValueStateUpdate) {
          return mergeFrom((org.apache.spark.sql.execution.streaming.state.StateMessage.ValueStateUpdate)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.spark.sql.execution.streaming.state.StateMessage.ValueStateUpdate other) {
        if (other == org.apache.spark.sql.execution.streaming.state.StateMessage.ValueStateUpdate.getDefaultInstance()) return this;
        if (other.getUpdatedValue() != com.google.protobuf.ByteString.EMPTY) {
          setUpdatedValue(other.getUpdatedValue());
        }
        this.mergeUnknownFields(other.getUnknownFields());
        onChanged();
        return this;
      }

      @java.lang.Override
      public final boolean isInitialized() {
        return true;
      }

      @java.lang.Override
      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        if (extensionRegistry == null) {
          throw new java.lang.NullPointerException();
        }
        try {
          boolean done = false;
          while (!done) {
            int tag = input.readTag();
            switch (tag) {
              case 0:
                done = true;
                break;
              case 10: {
                updatedValue_ = input.readBytes();
                bitField0_ |= 0x00000001;
                break;
              } // case 10
              default: {
                if (!super.parseUnknownField(input, extensionRegistry, tag)) {
                  done = true; // was an endgroup tag
                }
                break;
              } // default:
            } // switch (tag)
          } // while (!done)
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          throw e.unwrapIOException();
        } finally {
          onChanged();
        } // finally
        return this;
      }
      private int bitField0_;

      private com.google.protobuf.ByteString updatedValue_ = com.google.protobuf.ByteString.EMPTY;
      /**
       * <code>bytes updatedValue = 1;</code>
       * @return The updatedValue.
       */
      @java.lang.Override
      public com.google.protobuf.ByteString getUpdatedValue() {
        return updatedValue_;
      }
      /**
       * <code>bytes updatedValue = 1;</code>
       * @param value The updatedValue to set.
       * @return This builder for chaining.
       */
      public Builder setUpdatedValue(com.google.protobuf.ByteString value) {
        if (value == null) { throw new NullPointerException(); }
        updatedValue_ = value;
        bitField0_ |= 0x00000001;
        onChanged();
        return this;
      }
      /**
       * <code>bytes updatedValue = 1;</code>
       * @return This builder for chaining.
       */
      public Builder clearUpdatedValue() {
        bitField0_ = (bitField0_ & ~0x00000001);
        updatedValue_ = getDefaultInstance().getUpdatedValue();
        onChanged();
        return this;
      }
      @java.lang.Override
      public final Builder setUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.setUnknownFields(unknownFields);
      }

      @java.lang.Override
      public final Builder mergeUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.mergeUnknownFields(unknownFields);
      }


      // @@protoc_insertion_point(builder_scope:org.apache.spark.sql.execution.streaming.state.ValueStateUpdate)
    }

    // @@protoc_insertion_point(class_scope:org.apache.spark.sql.execution.streaming.state.ValueStateUpdate)
    private static final org.apache.spark.sql.execution.streaming.state.StateMessage.ValueStateUpdate DEFAULT_INSTANCE;
    static {
      DEFAULT_INSTANCE = new org.apache.spark.sql.execution.streaming.state.StateMessage.ValueStateUpdate();
    }

    public static org.apache.spark.sql.execution.streaming.state.StateMessage.ValueStateUpdate getDefaultInstance() {
      return DEFAULT_INSTANCE;
    }

    private static final com.google.protobuf.Parser<ValueStateUpdate>
        PARSER = new com.google.protobuf.AbstractParser<ValueStateUpdate>() {
      @java.lang.Override
      public ValueStateUpdate parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        Builder builder = newBuilder();
        try {
          builder.mergeFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          throw e.setUnfinishedMessage(builder.buildPartial());
        } catch (com.google.protobuf.UninitializedMessageException e) {
          throw e.asInvalidProtocolBufferException().setUnfinishedMessage(builder.buildPartial());
        } catch (java.io.IOException e) {
          throw new com.google.protobuf.InvalidProtocolBufferException(e)
              .setUnfinishedMessage(builder.buildPartial());
        }
        return builder.buildPartial();
      }
    };

    public static com.google.protobuf.Parser<ValueStateUpdate> parser() {
      return PARSER;
    }

    @java.lang.Override
    public com.google.protobuf.Parser<ValueStateUpdate> getParserForType() {
      return PARSER;
    }

    @java.lang.Override
    public org.apache.spark.sql.execution.streaming.state.StateMessage.ValueStateUpdate getDefaultInstanceForType() {
      return DEFAULT_INSTANCE;
    }

  }

  public interface SetHandleStateOrBuilder extends
      // @@protoc_insertion_point(interface_extends:org.apache.spark.sql.execution.streaming.state.SetHandleState)
      com.google.protobuf.MessageOrBuilder {

    /**
     * <code>.org.apache.spark.sql.execution.streaming.state.HandleState state = 1;</code>
     * @return The enum numeric value on the wire for state.
     */
    int getStateValue();
    /**
     * <code>.org.apache.spark.sql.execution.streaming.state.HandleState state = 1;</code>
     * @return The state.
     */
    org.apache.spark.sql.execution.streaming.state.StateMessage.HandleState getState();
  }
  /**
   * Protobuf type {@code org.apache.spark.sql.execution.streaming.state.SetHandleState}
   */
  public static final class SetHandleState extends
      com.google.protobuf.GeneratedMessageV3 implements
      // @@protoc_insertion_point(message_implements:org.apache.spark.sql.execution.streaming.state.SetHandleState)
      SetHandleStateOrBuilder {
  private static final long serialVersionUID = 0L;
    // Use SetHandleState.newBuilder() to construct.
    private SetHandleState(com.google.protobuf.GeneratedMessageV3.Builder<?> builder) {
      super(builder);
    }
    private SetHandleState() {
      state_ = 0;
    }

    @java.lang.Override
    @SuppressWarnings({"unused"})
    protected java.lang.Object newInstance(
        UnusedPrivateParameter unused) {
      return new SetHandleState();
    }

    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.spark.sql.execution.streaming.state.StateMessage.internal_static_org_apache_spark_sql_execution_streaming_state_SetHandleState_descriptor;
    }

    @java.lang.Override
    protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.spark.sql.execution.streaming.state.StateMessage.internal_static_org_apache_spark_sql_execution_streaming_state_SetHandleState_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.spark.sql.execution.streaming.state.StateMessage.SetHandleState.class, org.apache.spark.sql.execution.streaming.state.StateMessage.SetHandleState.Builder.class);
    }

    public static final int STATE_FIELD_NUMBER = 1;
    private int state_ = 0;
    /**
     * <code>.org.apache.spark.sql.execution.streaming.state.HandleState state = 1;</code>
     * @return The enum numeric value on the wire for state.
     */
    @java.lang.Override public int getStateValue() {
      return state_;
    }
    /**
     * <code>.org.apache.spark.sql.execution.streaming.state.HandleState state = 1;</code>
     * @return The state.
     */
    @java.lang.Override public org.apache.spark.sql.execution.streaming.state.StateMessage.HandleState getState() {
      org.apache.spark.sql.execution.streaming.state.StateMessage.HandleState result = org.apache.spark.sql.execution.streaming.state.StateMessage.HandleState.forNumber(state_);
      return result == null ? org.apache.spark.sql.execution.streaming.state.StateMessage.HandleState.UNRECOGNIZED : result;
    }

    private byte memoizedIsInitialized = -1;
    @java.lang.Override
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized == 1) return true;
      if (isInitialized == 0) return false;

      memoizedIsInitialized = 1;
      return true;
    }

    @java.lang.Override
    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      if (state_ != org.apache.spark.sql.execution.streaming.state.StateMessage.HandleState.CREATED.getNumber()) {
        output.writeEnum(1, state_);
      }
      getUnknownFields().writeTo(output);
    }

    @java.lang.Override
    public int getSerializedSize() {
      int size = memoizedSize;
      if (size != -1) return size;

      size = 0;
      if (state_ != org.apache.spark.sql.execution.streaming.state.StateMessage.HandleState.CREATED.getNumber()) {
        size += com.google.protobuf.CodedOutputStream
          .computeEnumSize(1, state_);
      }
      size += getUnknownFields().getSerializedSize();
      memoizedSize = size;
      return size;
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.spark.sql.execution.streaming.state.StateMessage.SetHandleState)) {
        return super.equals(obj);
      }
      org.apache.spark.sql.execution.streaming.state.StateMessage.SetHandleState other = (org.apache.spark.sql.execution.streaming.state.StateMessage.SetHandleState) obj;

      if (state_ != other.state_) return false;
      if (!getUnknownFields().equals(other.getUnknownFields())) return false;
      return true;
    }

    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptor().hashCode();
      hash = (37 * hash) + STATE_FIELD_NUMBER;
      hash = (53 * hash) + state_;
      hash = (29 * hash) + getUnknownFields().hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.spark.sql.execution.streaming.state.StateMessage.SetHandleState parseFrom(
        java.nio.ByteBuffer data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.spark.sql.execution.streaming.state.StateMessage.SetHandleState parseFrom(
        java.nio.ByteBuffer data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.spark.sql.execution.streaming.state.StateMessage.SetHandleState parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.spark.sql.execution.streaming.state.StateMessage.SetHandleState parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.spark.sql.execution.streaming.state.StateMessage.SetHandleState parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.spark.sql.execution.streaming.state.StateMessage.SetHandleState parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.spark.sql.execution.streaming.state.StateMessage.SetHandleState parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.spark.sql.execution.streaming.state.StateMessage.SetHandleState parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }

    public static org.apache.spark.sql.execution.streaming.state.StateMessage.SetHandleState parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input);
    }

    public static org.apache.spark.sql.execution.streaming.state.StateMessage.SetHandleState parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.spark.sql.execution.streaming.state.StateMessage.SetHandleState parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.spark.sql.execution.streaming.state.StateMessage.SetHandleState parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }

    @java.lang.Override
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder() {
      return DEFAULT_INSTANCE.toBuilder();
    }
    public static Builder newBuilder(org.apache.spark.sql.execution.streaming.state.StateMessage.SetHandleState prototype) {
      return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
    }
    @java.lang.Override
    public Builder toBuilder() {
      return this == DEFAULT_INSTANCE
          ? new Builder() : new Builder().mergeFrom(this);
    }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code org.apache.spark.sql.execution.streaming.state.SetHandleState}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessageV3.Builder<Builder> implements
        // @@protoc_insertion_point(builder_implements:org.apache.spark.sql.execution.streaming.state.SetHandleState)
        org.apache.spark.sql.execution.streaming.state.StateMessage.SetHandleStateOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.spark.sql.execution.streaming.state.StateMessage.internal_static_org_apache_spark_sql_execution_streaming_state_SetHandleState_descriptor;
      }

      @java.lang.Override
      protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.spark.sql.execution.streaming.state.StateMessage.internal_static_org_apache_spark_sql_execution_streaming_state_SetHandleState_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.spark.sql.execution.streaming.state.StateMessage.SetHandleState.class, org.apache.spark.sql.execution.streaming.state.StateMessage.SetHandleState.Builder.class);
      }

      // Construct using org.apache.spark.sql.execution.streaming.state.StateMessage.SetHandleState.newBuilder()
      private Builder() {

      }

      private Builder(
          com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
        super(parent);

      }
      @java.lang.Override
      public Builder clear() {
        super.clear();
        bitField0_ = 0;
        state_ = 0;
        return this;
      }

      @java.lang.Override
      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.spark.sql.execution.streaming.state.StateMessage.internal_static_org_apache_spark_sql_execution_streaming_state_SetHandleState_descriptor;
      }

      @java.lang.Override
      public org.apache.spark.sql.execution.streaming.state.StateMessage.SetHandleState getDefaultInstanceForType() {
        return org.apache.spark.sql.execution.streaming.state.StateMessage.SetHandleState.getDefaultInstance();
      }

      @java.lang.Override
      public org.apache.spark.sql.execution.streaming.state.StateMessage.SetHandleState build() {
        org.apache.spark.sql.execution.streaming.state.StateMessage.SetHandleState result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      @java.lang.Override
      public org.apache.spark.sql.execution.streaming.state.StateMessage.SetHandleState buildPartial() {
        org.apache.spark.sql.execution.streaming.state.StateMessage.SetHandleState result = new org.apache.spark.sql.execution.streaming.state.StateMessage.SetHandleState(this);
        if (bitField0_ != 0) { buildPartial0(result); }
        onBuilt();
        return result;
      }

      private void buildPartial0(org.apache.spark.sql.execution.streaming.state.StateMessage.SetHandleState result) {
        int from_bitField0_ = bitField0_;
        if (((from_bitField0_ & 0x00000001) != 0)) {
          result.state_ = state_;
        }
      }

      @java.lang.Override
      public Builder clone() {
        return super.clone();
      }
      @java.lang.Override
      public Builder setField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.setField(field, value);
      }
      @java.lang.Override
      public Builder clearField(
          com.google.protobuf.Descriptors.FieldDescriptor field) {
        return super.clearField(field);
      }
      @java.lang.Override
      public Builder clearOneof(
          com.google.protobuf.Descriptors.OneofDescriptor oneof) {
        return super.clearOneof(oneof);
      }
      @java.lang.Override
      public Builder setRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          int index, java.lang.Object value) {
        return super.setRepeatedField(field, index, value);
      }
      @java.lang.Override
      public Builder addRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.addRepeatedField(field, value);
      }
      @java.lang.Override
      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.spark.sql.execution.streaming.state.StateMessage.SetHandleState) {
          return mergeFrom((org.apache.spark.sql.execution.streaming.state.StateMessage.SetHandleState)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.spark.sql.execution.streaming.state.StateMessage.SetHandleState other) {
        if (other == org.apache.spark.sql.execution.streaming.state.StateMessage.SetHandleState.getDefaultInstance()) return this;
        if (other.state_ != 0) {
          setStateValue(other.getStateValue());
        }
        this.mergeUnknownFields(other.getUnknownFields());
        onChanged();
        return this;
      }

      @java.lang.Override
      public final boolean isInitialized() {
        return true;
      }

      @java.lang.Override
      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        if (extensionRegistry == null) {
          throw new java.lang.NullPointerException();
        }
        try {
          boolean done = false;
          while (!done) {
            int tag = input.readTag();
            switch (tag) {
              case 0:
                done = true;
                break;
              case 8: {
                state_ = input.readEnum();
                bitField0_ |= 0x00000001;
                break;
              } // case 8
              default: {
                if (!super.parseUnknownField(input, extensionRegistry, tag)) {
                  done = true; // was an endgroup tag
                }
                break;
              } // default:
            } // switch (tag)
          } // while (!done)
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          throw e.unwrapIOException();
        } finally {
          onChanged();
        } // finally
        return this;
      }
      private int bitField0_;

      private int state_ = 0;
      /**
       * <code>.org.apache.spark.sql.execution.streaming.state.HandleState state = 1;</code>
       * @return The enum numeric value on the wire for state.
       */
      @java.lang.Override public int getStateValue() {
        return state_;
      }
      /**
       * <code>.org.apache.spark.sql.execution.streaming.state.HandleState state = 1;</code>
       * @param value The enum numeric value on the wire for state to set.
       * @return This builder for chaining.
       */
      public Builder setStateValue(int value) {
        state_ = value;
        bitField0_ |= 0x00000001;
        onChanged();
        return this;
      }
      /**
       * <code>.org.apache.spark.sql.execution.streaming.state.HandleState state = 1;</code>
       * @return The state.
       */
      @java.lang.Override
      public org.apache.spark.sql.execution.streaming.state.StateMessage.HandleState getState() {
        org.apache.spark.sql.execution.streaming.state.StateMessage.HandleState result = org.apache.spark.sql.execution.streaming.state.StateMessage.HandleState.forNumber(state_);
        return result == null ? org.apache.spark.sql.execution.streaming.state.StateMessage.HandleState.UNRECOGNIZED : result;
      }
      /**
       * <code>.org.apache.spark.sql.execution.streaming.state.HandleState state = 1;</code>
       * @param value The state to set.
       * @return This builder for chaining.
       */
      public Builder setState(org.apache.spark.sql.execution.streaming.state.StateMessage.HandleState value) {
        if (value == null) {
          throw new NullPointerException();
        }
        bitField0_ |= 0x00000001;
        state_ = value.getNumber();
        onChanged();
        return this;
      }
      /**
       * <code>.org.apache.spark.sql.execution.streaming.state.HandleState state = 1;</code>
       * @return This builder for chaining.
       */
      public Builder clearState() {
        bitField0_ = (bitField0_ & ~0x00000001);
        state_ = 0;
        onChanged();
        return this;
      }
      @java.lang.Override
      public final Builder setUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.setUnknownFields(unknownFields);
      }

      @java.lang.Override
      public final Builder mergeUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.mergeUnknownFields(unknownFields);
      }


      // @@protoc_insertion_point(builder_scope:org.apache.spark.sql.execution.streaming.state.SetHandleState)
    }

    // @@protoc_insertion_point(class_scope:org.apache.spark.sql.execution.streaming.state.SetHandleState)
    private static final org.apache.spark.sql.execution.streaming.state.StateMessage.SetHandleState DEFAULT_INSTANCE;
    static {
      DEFAULT_INSTANCE = new org.apache.spark.sql.execution.streaming.state.StateMessage.SetHandleState();
    }

    public static org.apache.spark.sql.execution.streaming.state.StateMessage.SetHandleState getDefaultInstance() {
      return DEFAULT_INSTANCE;
    }

    private static final com.google.protobuf.Parser<SetHandleState>
        PARSER = new com.google.protobuf.AbstractParser<SetHandleState>() {
      @java.lang.Override
      public SetHandleState parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        Builder builder = newBuilder();
        try {
          builder.mergeFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          throw e.setUnfinishedMessage(builder.buildPartial());
        } catch (com.google.protobuf.UninitializedMessageException e) {
          throw e.asInvalidProtocolBufferException().setUnfinishedMessage(builder.buildPartial());
        } catch (java.io.IOException e) {
          throw new com.google.protobuf.InvalidProtocolBufferException(e)
              .setUnfinishedMessage(builder.buildPartial());
        }
        return builder.buildPartial();
      }
    };

    public static com.google.protobuf.Parser<SetHandleState> parser() {
      return PARSER;
    }

    @java.lang.Override
    public com.google.protobuf.Parser<SetHandleState> getParserForType() {
      return PARSER;
    }

    @java.lang.Override
    public org.apache.spark.sql.execution.streaming.state.StateMessage.SetHandleState getDefaultInstanceForType() {
      return DEFAULT_INSTANCE;
    }

  }

  private static final com.google.protobuf.Descriptors.Descriptor
    internal_static_org_apache_spark_sql_execution_streaming_state_StateRequest_descriptor;
  private static final 
    com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_org_apache_spark_sql_execution_streaming_state_StateRequest_fieldAccessorTable;
  private static final com.google.protobuf.Descriptors.Descriptor
    internal_static_org_apache_spark_sql_execution_streaming_state_StatefulProcessorHandleCall_descriptor;
  private static final 
    com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_org_apache_spark_sql_execution_streaming_state_StatefulProcessorHandleCall_fieldAccessorTable;
  private static final com.google.protobuf.Descriptors.Descriptor
    internal_static_org_apache_spark_sql_execution_streaming_state_GetValueState_descriptor;
  private static final 
    com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_org_apache_spark_sql_execution_streaming_state_GetValueState_fieldAccessorTable;
  private static final com.google.protobuf.Descriptors.Descriptor
    internal_static_org_apache_spark_sql_execution_streaming_state_ValueStateCall_descriptor;
  private static final 
    com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_org_apache_spark_sql_execution_streaming_state_ValueStateCall_fieldAccessorTable;
  private static final com.google.protobuf.Descriptors.Descriptor
    internal_static_org_apache_spark_sql_execution_streaming_state_ValueStateExists_descriptor;
  private static final 
    com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_org_apache_spark_sql_execution_streaming_state_ValueStateExists_fieldAccessorTable;
  private static final com.google.protobuf.Descriptors.Descriptor
    internal_static_org_apache_spark_sql_execution_streaming_state_ValueStateGet_descriptor;
  private static final 
    com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_org_apache_spark_sql_execution_streaming_state_ValueStateGet_fieldAccessorTable;
  private static final com.google.protobuf.Descriptors.Descriptor
    internal_static_org_apache_spark_sql_execution_streaming_state_ValueStateUpdate_descriptor;
  private static final 
    com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_org_apache_spark_sql_execution_streaming_state_ValueStateUpdate_fieldAccessorTable;
  private static final com.google.protobuf.Descriptors.Descriptor
    internal_static_org_apache_spark_sql_execution_streaming_state_SetHandleState_descriptor;
  private static final 
    com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_org_apache_spark_sql_execution_streaming_state_SetHandleState_fieldAccessorTable;

  public static com.google.protobuf.Descriptors.FileDescriptor
      getDescriptor() {
    return descriptor;
  }
  private static  com.google.protobuf.Descriptors.FileDescriptor
      descriptor;
  static {
    java.lang.String[] descriptorData = {
      "\n;org/apache/spark/sql/execution/streami" +
      "ng/StateMessage.proto\022.org.apache.spark." +
      "sql.execution.streaming.state\"\367\001\n\014StateR" +
      "equest\022\017\n\007version\030\001 \001(\005\022r\n\033statefulProce" +
      "ssorHandleCall\030\002 \001(\0132K.org.apache.spark." +
      "sql.execution.streaming.state.StatefulPr" +
      "ocessorHandleCallH\000\022X\n\016valueStateCall\030\003 " +
      "\001(\0132>.org.apache.spark.sql.execution.str" +
      "eaming.state.ValueStateCallH\000B\010\n\006method\"" +
      "\331\001\n\033StatefulProcessorHandleCall\022X\n\016setHa" +
      "ndleState\030\002 \001(\0132>.org.apache.spark.sql.e" +
      "xecution.streaming.state.SetHandleStateH" +
      "\000\022V\n\rgetValueState\030\003 \001(\0132=.org.apache.sp" +
      "ark.sql.execution.streaming.state.GetVal" +
      "ueStateH\000B\010\n\006method\"\"\n\rGetValueState\022\021\n\t" +
      "stateName\030\001 \001(\t\"\220\002\n\016ValueStateCall\022R\n\006ex" +
      "ists\030\001 \001(\0132@.org.apache.spark.sql.execut" +
      "ion.streaming.state.ValueStateExistsH\000\022L" +
      "\n\003get\030\002 \001(\0132=.org.apache.spark.sql.execu" +
      "tion.streaming.state.ValueStateGetH\000\022R\n\006" +
      "update\030\003 \001(\0132@.org.apache.spark.sql.exec" +
      "ution.streaming.state.ValueStateUpdateH\000" +
      "B\010\n\006method\"\022\n\020ValueStateExists\"\017\n\rValueS" +
      "tateGet\"(\n\020ValueStateUpdate\022\024\n\014updatedVa" +
      "lue\030\001 \001(\014\"\\\n\016SetHandleState\022J\n\005state\030\001 \001" +
      "(\0162;.org.apache.spark.sql.execution.stre" +
      "aming.state.HandleState*K\n\013HandleState\022\013" +
      "\n\007CREATED\020\000\022\017\n\013INITIALIZED\020\001\022\022\n\016DATA_PRO" +
      "CESSED\020\002\022\n\n\006CLOSED\020\003b\006proto3"
    };
    descriptor = com.google.protobuf.Descriptors.FileDescriptor
      .internalBuildGeneratedFileFrom(descriptorData,
        new com.google.protobuf.Descriptors.FileDescriptor[] {
        });
    internal_static_org_apache_spark_sql_execution_streaming_state_StateRequest_descriptor =
      getDescriptor().getMessageTypes().get(0);
    internal_static_org_apache_spark_sql_execution_streaming_state_StateRequest_fieldAccessorTable = new
      com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_org_apache_spark_sql_execution_streaming_state_StateRequest_descriptor,
        new java.lang.String[] { "Version", "StatefulProcessorHandleCall", "ValueStateCall", "Method", });
    internal_static_org_apache_spark_sql_execution_streaming_state_StatefulProcessorHandleCall_descriptor =
      getDescriptor().getMessageTypes().get(1);
    internal_static_org_apache_spark_sql_execution_streaming_state_StatefulProcessorHandleCall_fieldAccessorTable = new
      com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_org_apache_spark_sql_execution_streaming_state_StatefulProcessorHandleCall_descriptor,
        new java.lang.String[] { "SetHandleState", "GetValueState", "Method", });
    internal_static_org_apache_spark_sql_execution_streaming_state_GetValueState_descriptor =
      getDescriptor().getMessageTypes().get(2);
    internal_static_org_apache_spark_sql_execution_streaming_state_GetValueState_fieldAccessorTable = new
      com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_org_apache_spark_sql_execution_streaming_state_GetValueState_descriptor,
        new java.lang.String[] { "StateName", });
    internal_static_org_apache_spark_sql_execution_streaming_state_ValueStateCall_descriptor =
      getDescriptor().getMessageTypes().get(3);
    internal_static_org_apache_spark_sql_execution_streaming_state_ValueStateCall_fieldAccessorTable = new
      com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_org_apache_spark_sql_execution_streaming_state_ValueStateCall_descriptor,
        new java.lang.String[] { "Exists", "Get", "Update", "Method", });
    internal_static_org_apache_spark_sql_execution_streaming_state_ValueStateExists_descriptor =
      getDescriptor().getMessageTypes().get(4);
    internal_static_org_apache_spark_sql_execution_streaming_state_ValueStateExists_fieldAccessorTable = new
      com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_org_apache_spark_sql_execution_streaming_state_ValueStateExists_descriptor,
        new java.lang.String[] { });
    internal_static_org_apache_spark_sql_execution_streaming_state_ValueStateGet_descriptor =
      getDescriptor().getMessageTypes().get(5);
    internal_static_org_apache_spark_sql_execution_streaming_state_ValueStateGet_fieldAccessorTable = new
      com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_org_apache_spark_sql_execution_streaming_state_ValueStateGet_descriptor,
        new java.lang.String[] { });
    internal_static_org_apache_spark_sql_execution_streaming_state_ValueStateUpdate_descriptor =
      getDescriptor().getMessageTypes().get(6);
    internal_static_org_apache_spark_sql_execution_streaming_state_ValueStateUpdate_fieldAccessorTable = new
      com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_org_apache_spark_sql_execution_streaming_state_ValueStateUpdate_descriptor,
        new java.lang.String[] { "UpdatedValue", });
    internal_static_org_apache_spark_sql_execution_streaming_state_SetHandleState_descriptor =
      getDescriptor().getMessageTypes().get(7);
    internal_static_org_apache_spark_sql_execution_streaming_state_SetHandleState_fieldAccessorTable = new
      com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_org_apache_spark_sql_execution_streaming_state_SetHandleState_descriptor,
        new java.lang.String[] { "State", });
  }

  // @@protoc_insertion_point(outer_class_scope)
}
